[
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "üéâ Thank you for checking out our project! üéâ\nThis page contains guidelines on how to get in touch with us and potentially contribute towards this repository.\n\n\nYou can contact the researchers on this project using the provided email addresses in CITATION.cff.\n\n\n\nIf you spot an issue, you are welcome to raise this either by:\n\nUsing GitHub Issues.\nForking the repository, make your changes and submit a pull request for review."
  },
  {
    "objectID": "CONTRIBUTING.html#email",
    "href": "CONTRIBUTING.html#email",
    "title": "Contributing",
    "section": "",
    "text": "You can contact the researchers on this project using the provided email addresses in CITATION.cff."
  },
  {
    "objectID": "CONTRIBUTING.html#suggesting-changes",
    "href": "CONTRIBUTING.html#suggesting-changes",
    "title": "Contributing",
    "section": "",
    "text": "If you spot an issue, you are welcome to raise this either by:\n\nUsing GitHub Issues.\nForking the repository, make your changes and submit a pull request for review."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html",
    "href": "logbook/posts/2024_07_31/index.html",
    "title": "Day 4",
    "section": "",
    "text": "Note\n\n\n\nReproduced Figures 2-4, subset of Table 3, and ran some of the other scenarios. Total time used: 12h 21m (30.9%)."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#run-surv-scenario-2",
    "href": "logbook/posts/2024_07_31/index.html#run-surv-scenario-2",
    "title": "Day 4",
    "section": "09.12-09.16: Run surv scenario 2",
    "text": "09.12-09.16: Run surv scenario 2\nUpdate renv on remote machine, and then set surv scen2 to run.\nTimings:\n\nFirst run: 14.850 minutes = 891 seconds = 14 minutes 51 seconds\nAll runs: 99.651 minutes = 5979 seconds = 1 hour 39 minutes 39 seconds"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#figure-2",
    "href": "logbook/posts/2024_07_31/index.html#figure-2",
    "title": "Day 4",
    "section": "09.20-09.58: Figure 2",
    "text": "09.20-09.58: Figure 2\nFixed dimensions of Figure 1 so it doesn‚Äôt change each time I re-run with different window widths. Then resumed work on Figure 2, adapting Figure 1 code to create functions that can be used to produce both plots.\nSatisfied this is reproduced at 09.58.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 539\n\n# Times from today\ntimes = [\n    ('09.12', '09.16'),\n    ('09.20', '09.58')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 42m, or 0h 42m\nTotal used to date: 581m, or 9h 41m\nTime remaining: 1819m, or 30h 19m\nUsed 24.2% of 40 hours max\n\n\n\n\n\n\n\n\nReflection\n\n\n\nNice simple output from the model makes it easier to work with, as well as it being simple and similar plots with fairly minimal manipulation."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#figure-3-and-table-3",
    "href": "logbook/posts/2024_07_31/index.html#figure-3-and-table-3",
    "title": "Day 4",
    "section": "10.09-10.27: Figure 3 and Table 3",
    "text": "10.09-10.27: Figure 3 and Table 3\nUsing surv scen 1 (as in figure legend and article), starting working on Figure 3 and part of Table 3, adapting my prior functions.\nI realised the output for scenario 1 doesn‚Äôt include a result from delay 0, so I will need the outputs from the base scenario scen0 to create this plot. I have not yet generated these.\nThis will also need different scaling. As in the Table 3 caption:\n\nNational surveillance cohort March 2020: n=15376\nExpected AAA deaths in status quo: n=2152\nExpected emergency operations in status quo: n=745"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#set-surv-scenario-0-to-run-on-remote-machine",
    "href": "logbook/posts/2024_07_31/index.html#set-surv-scenario-0-to-run-on-remote-machine",
    "title": "Day 4",
    "section": "10.56-11.02: Set surv scenario 0 to run on remote machine",
    "text": "10.56-11.02: Set surv scenario 0 to run on remote machine\nFirst had to push and pull changes, and then update surv scenario 0 R script (as we have done for others) (parallel, 1 million, csv, etc.)\nTimings:\n\n4.463 minutes = 268 seconds = 4 minutes 28 seconds"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#set-surv-scenario-3-to-run-on-remote-machine",
    "href": "logbook/posts/2024_07_31/index.html#set-surv-scenario-3-to-run-on-remote-machine",
    "title": "Day 4",
    "section": "11.06-11.07: Set surv scenario 3 to run on remote machine",
    "text": "11.06-11.07: Set surv scenario 3 to run on remote machine\nTimings:\n\nFirst run: 13.303 minutes = 798 seconds = 13 minutes 18 seconds\nFull run: 81.663 minutes = 4900 seconds = 81 minutes 40 seconds"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#returning-to-figure-3-and-a-subset-of-table-3",
    "href": "logbook/posts/2024_07_31/index.html#returning-to-figure-3-and-a-subset-of-table-3",
    "title": "Day 4",
    "section": "11.11-11.24: Returning to Figure 3 and a subset of Table 3",
    "text": "11.11-11.24: Returning to Figure 3 and a subset of Table 3\nWith scenario 0 results, I could finish making the subset for Table 3, and completed Figure 3.\nSatisfied Figure 3 is reproduced at 11.24.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 539\n\n# Times from today\ntimes = [\n    ('09.12', '09.16'),\n    ('09.20', '09.58'),\n    ('10.09', '10.27'),\n    ('10.56', '11.02'),\n    ('11.06', '11.07'),\n    ('11.11', '11.24')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 80m, or 1h 20m\nTotal used to date: 619m, or 10h 19m\nTime remaining: 1781m, or 29h 41m\nUsed 25.8% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#working-on-figure-4",
    "href": "logbook/posts/2024_07_31/index.html#working-on-figure-4",
    "title": "Day 4",
    "section": "11.31-11.55, 12.43-13.00: Working on Figure 4",
    "text": "11.31-11.55, 12.43-13.00: Working on Figure 4\nSurv_scen2 includes s2.1 and s2.2, as it varies the dropout rate for 1 year, or for 2 years. These are each in part of Table 3, and together in Figure 4."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#set-up-surv-scen-4a-to-run",
    "href": "logbook/posts/2024_07_31/index.html#set-up-surv-scen-4a-to-run",
    "title": "Day 4",
    "section": "13.01-13.04: Set up surv scen 4a to run",
    "text": "13.01-13.04: Set up surv scen 4a to run\nTimings:\n\nFirst run: forgot to note down\nAll runs: 35.390 min = 2123 seconds = 35 minutes 23 seconds"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#resuming-figure-4",
    "href": "logbook/posts/2024_07_31/index.html#resuming-figure-4",
    "title": "Day 4",
    "section": "13.05-13.14: Resuming Figure 4",
    "text": "13.05-13.14: Resuming Figure 4\nHad to combine with scenario 0 again to give result when dropout rate is 0.\nSatisfied this is reproduced at 13.14.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 539\n\n# Times from today\ntimes = [\n    ('09.12', '09.16'),\n    ('09.20', '09.58'),\n    ('10.09', '10.27'),\n    ('10.56', '11.02'),\n    ('11.06', '11.07'),\n    ('11.11', '11.24'),\n    ('11.31', '11.55'),\n    ('12.43', '13.00'),\n    ('13.01', '13.04'),\n    ('13.05', '13.14')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 133m, or 2h 13m\nTotal used to date: 672m, or 11h 12m\nTime remaining: 1728m, or 28h 48m\nUsed 28.0% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#subset-of-table-3",
    "href": "logbook/posts/2024_07_31/index.html#subset-of-table-3",
    "title": "Day 4",
    "section": "13.22-13.29: Subset of Table 3",
    "text": "13.22-13.29: Subset of Table 3\nCreated the scenario 2 sections of table 3 using my prior functions.\n\n\n\n\n\n\nReflection\n\n\n\nGiven small size of these results dataframe, could have been handy to include them along with the code."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#in-text-result-1",
    "href": "logbook/posts/2024_07_31/index.html#in-text-result-1",
    "title": "Day 4",
    "section": "13.32-13.40: In-text result 1",
    "text": "13.32-13.40: In-text result 1\nThe output from the model for Figure 3/Table 3 (..._surv_scen1.R) just returns total AAA deaths and not which sub-groups these belonged to.\nI couldn‚Äôt spot any code in the repository that would extra this result by these sub-groups, so set about identifying how to do this."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#set-up-surv-scen4b-to-run",
    "href": "logbook/posts/2024_07_31/index.html#set-up-surv-scen4b-to-run",
    "title": "Day 4",
    "section": "13.53-13.55: Set up surv scen4b to run",
    "text": "13.53-13.55: Set up surv scen4b to run\nPush and pull, and set to run.\n\n\n\n\n\n\nReflection\n\n\n\nThe original study authors inclusion of seeds and parallel processing is great and really handy."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#return-to-in-text-result-1",
    "href": "logbook/posts/2024_07_31/index.html#return-to-in-text-result-1",
    "title": "Day 4",
    "section": "13.56-14.00, 14.10-14.49, 16.07-16.16: Return to in-text result 1",
    "text": "13.56-14.00, 14.10-14.49, 16.07-16.16: Return to in-text result 1\nI ran a simple version of the model on my machine (few people) so I could inspect the scen1.surv object returned.\nscen1.surv$eventHistories is a list where each item is a person from the simulation. We can get:\n\nWhether they died - if ‚ÄúaaaDeath‚Äù is in their events (scen1.surv$eventHistories[[i]]$screening$events)\nTheir aaorta size - scen1.surv$eventHistories[[i]]$screening$initialAortaSizeAsMeasured\n\nI wrote a function that extracts the sizes for people who have died and counts numbers in each category. I add this to ..._surv_scen1.R, initially test running with a small number of people. This worked fine, so I then set it to run on the remote machine.\nTimings:\n\nOne run: 13.346 minutes = 801 seconds = 13 minutes 21 seconds\nAll runs: 68.854 minutes = 4131 seconds = 1 hour 8 minutes 51 seconds\n\nI imported the results, and then realised I would also need to run this function with ..._surv_scen0.R."
  },
  {
    "objectID": "logbook/posts/2024_07_31/index.html#timings",
    "href": "logbook/posts/2024_07_31/index.html#timings",
    "title": "Day 4",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 539\n\n# Times from today\ntimes = [\n    ('09.12', '09.16'),\n    ('09.20', '09.58'),\n    ('10.09', '10.27'),\n    ('10.56', '11.02'),\n    ('11.06', '11.07'),\n    ('11.11', '11.24'),\n    ('11.31', '11.55'),\n    ('12.43', '13.00'),\n    ('13.01', '13.04'),\n    ('13.05', '13.14'),\n    ('13.22', '13.29'),\n    ('13.32', '13.40'),\n    ('13.53', '13.55'),\n    ('13.56', '14.00'),\n    ('14.10', '14.49'),\n    ('16.07', '16.16')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 202m, or 3h 22m\nTotal used to date: 741m, or 12h 21m\nTime remaining: 1659m, or 27h 39m\nUsed 30.9% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html",
    "href": "logbook/posts/2024_07_29/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Note\n\n\n\nRead article, decided on scope, then began running model. Total time used: 4h 3m (10.1%)."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#high-entropy-secret",
    "href": "logbook/posts/2024_07_29/index.html#high-entropy-secret",
    "title": "Day 2",
    "section": "09.38-09.43: High entropy secret",
    "text": "09.38-09.43: High entropy secret\nUpload of code to repository triggered a notification from GitGuardian for upload of a high entropy secret. Opening the GitGuardian dashboard, I found the source of this issue was an API key uploaded to original_study/AAA_DES_model/input/NAAASP_Men_2020-05-11/DES_Data_Input_NAAASP_Men_30years_time_horizon_2020-05-11.R on line 376:\n# SOURCE: Love-Koh 2015 (https://reader.elsevier.com/reader/sd/pii/S1098301515018471?token=AB11057F469D57EBE990778CCDC883E9D35D3CFD83AD143352F3AD497E822198F957DBCE2EA86AE6DC16F2F6CE350409)\nThis is not a concern - the link doesn‚Äôt work with that token for others.\nThe DOI for this article is https://doi.org/10.1016/j.jval.2015.03.1784."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#correct-file-paths",
    "href": "logbook/posts/2024_07_29/index.html#correct-file-paths",
    "title": "Day 2",
    "section": "09.53-09.55: Correct file paths",
    "text": "09.53-09.55: Correct file paths\nFor study_publication.qmd."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#read-journal-article",
    "href": "logbook/posts/2024_07_29/index.html#read-journal-article",
    "title": "Day 2",
    "section": "10.05-10.15, 10.29-10.45: Read journal article",
    "text": "10.05-10.15, 10.29-10.45: Read journal article\nUses a previously developed and validated model described in:\n\nGlover MJ, Jones E, Masconi KL, Sweeting MJ, Thompson SG. Discrete Event Simulation for Decision Modeling in Health Care: Lessons from Abdominal Aortic Aneurysm Screening. Med Decis Making. 2018; 38(4):439‚Äì51. https://doi.org/10.1177/0272989X17753380 PMID: 31665967\nThompson SG, Bown MJ, Glover MJ, Jones E, Masconi KL, Michaels JA, et al.¬†Screening women aged 65 years or over for abdominal aortic aneurysm: a modelling study and health economic evaluation. Health Technol Assess. 2018; 22(43):1‚Äì142. https://doi.org/10.3310/hta22430 PMID: 30132754\n\nSome notes on scope (beyond obvious tables/figures):\n\nIn-text result for Surveillance cohort: Scan suspension (re: breakdown of deaths by type) are not captured in tables/figures\nSupplementary figure 1 and 2 are more like methods than results? Are referenced in methods.\nIn-text results for Surveillance cohort: Drop-out are captured in Table 3 and Figure 4\nAlso captured are Surveillance cohort: Threshold for surgery\nSupplementary figure 3 and supplementary table 2 are definitely in scope"
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#define-scope-for-reproduction",
    "href": "logbook/posts/2024_07_29/index.html#define-scope-for-reproduction",
    "title": "Day 2",
    "section": "10.46-11.02: Define scope for reproduction",
    "text": "10.46-11.02: Define scope for reproduction\nI described my interpretation of scope within scope.qmd."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#discussed-scope-with-tom",
    "href": "logbook/posts/2024_07_29/index.html#discussed-scope-with-tom",
    "title": "Day 2",
    "section": "11.09-11.13: Discussed scope with Tom",
    "text": "11.09-11.13: Discussed scope with Tom\nLooked over and happy with scope. He then did another look over the paper, but confirmed no further items."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#archive-scope",
    "href": "logbook/posts/2024_07_29/index.html#archive-scope",
    "title": "Day 2",
    "section": "11.26-11.32 : Archive scope",
    "text": "11.26-11.32 : Archive scope\nUpdate CHANGELOG.md and CITATION.cff, set up sync on Zenodo, then created a Git release."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#look-over-code",
    "href": "logbook/posts/2024_07_29/index.html#look-over-code",
    "title": "Day 2",
    "section": "12.50-12.56: Look over code",
    "text": "12.50-12.56: Look over code\nREADME:\n\nDirects to NAASP_DES_Example_Script.R which provides walk through example for running model.\nNotes the repository contains code for two papers - we are interested in NAAASP_COVID (rather than SWAN)\n\nCopied the items relevant to NAAASP COVID into reproduction/. As README states results are saved to output/, create a placeholder output folder\nThe models/ folder seems to contain scripts to run each of the scenarios."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#set-up-environment",
    "href": "logbook/posts/2024_07_29/index.html#set-up-environment",
    "title": "Day 2",
    "section": "12.57-13.04: Set up environment",
    "text": "12.57-13.04: Set up environment\nBased on experience trying to backdate R and packages in my previous reproduction (Huang et al.¬†2019), I decided that this time I would start with the latest R and packages, and then backdate if its not working.\nThis means using R 4.4.1 (paper mentions 3.6.3).\nAt the start of each script, it lists the packages to be installed (though not versions). Created a DESCRIPTION file with these packages.\nTitle: Computational reproducibility assessment of Kim et al. 2019\nDepends: \n    R\nImports:\n    Rcpp,\n    expm,\n    msm,\n    foreach,\n    iterators,\n    doParallel\nRan:\nrenv::init(bare=TRUE)\nrenv::install()\nrenv::snapshot()"
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#trying-to-run-model",
    "href": "logbook/posts/2024_07_29/index.html#trying-to-run-model",
    "title": "Day 2",
    "section": "13.05-13.13, 13.48-13.55, 14.07-14.21, 14.53-15.08: Trying to run model",
    "text": "13.05-13.13, 13.48-13.55, 14.07-14.21, 14.53-15.08: Trying to run model\nTried running run_aaamodel_65yo_scen0.R. Had error:\nthis.dir &lt;- dirname(parent.frame(2)$ofile)\nError in dirname(parent.frame(2)$ofile) : \n  a character vector argument expected\nRealised this has a comment above stating:\n# Set the working directory to the root directory of AAA_DES_model (can only run this if sourcing the file)\nSo instead tried running with Source - this worked, up until this line:\n&gt; v1other\npostSurgeryInitialPeriod = 0.08213552 \ntimeToMonitoringFollowingOpenSurgery = 0.1149897 \ntimeBetweenMonitoringFollowingEvarSurgery = 1 \nzeroGrowthDiameterThreshold = 2 \nbaselineDiameters = data.frame with 2 columns:\n  size = 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 ...\n  weight = 2.86e-06 5.71e-06 4.43e-05 0.000351429 0.001205714 0.005032857 0.017921429 0.045698572 ...\nprevalenceThreshold = 3 \naortaDiameterThresholds = list: =Error in cat(names(element)[i], \"=\", element[[i]], \" \", sep = \"\") : \n  argument 3 (type 'list') cannot be handled by 'cat'\nv1other was created when we called source(\"input/NAAASP_Men_2020-05-11/DES_Data_Input_NAAASP_Men_30years_time_horizon_2020-05-11.R\"). Looking at that script, aortaDiameterThresholds are set as v1other$aortaDiameterThresholds &lt;- list(c(3.0, 4.5, 5.5)). Viewing the object rather than printing it with View(v1other), I can see that the thresholds are stored, but its just a list within a list.\nI tried commenting the print statement in run_aaamodel... but this just led to a later error for the same variable when it is used. As such, I instead altered assignment of this variable in the sourced script to c(3.0, 4.5, 5.5) (removing list()).\nThis then ran without issue. As it ran, I can see it sets a random seed (set.seed(3210)). As per usual, paused timing while it ran.\nSpent a long while on processPersons() (scen0.invite &lt;- processPersons(v0, v1other, v2, personData.screen)). At 13.48 (with model running since 13.13), I could see these would likely be slow models to run, so logged into the remote computer, cloned the GitHub repository there, and ran the next script starting at 13.55, run_aaamodel_65yo_scen0.R. To do so, after cloning, I ran:\ncd stars-reproduce-kim-2021/reproduction\nRscript -e \"renv::restore()\"\nRscript -e \"source('models/NAAASP_COVID_modelling/run_aaamodel_65yo_scen1.R')\"\n\n\n\n\n\n\nReflection\n\n\n\nIncluding the expected run time would be handy, as I assume this is working fine, but that‚Äôs working on the assumption that these models take a long time to run (which I‚Äôm not sure yet whether that is the case or not).\nIn this case, as I later found out they used HPC, it would‚Äôve been beneficial to mention that also!\n\n\nI checked the run_aaamodel... scripts and realised that in each, none were using parallel processing (as v0$method &lt;- \"serial\").\nLooking at the DES_Model code I can see the example of processPersons() allows three possible inputs:\n\n‚Äúserial‚Äù\n‚Äúparallel‚Äù (which uses parallel)\n‚Äúforeach‚Äù (which uses doParallel)\n\nThe function psa() is the same, but also includes ‚ÄúparallelBatches‚Äù alongside ‚Äúforeach‚Äù. However, the consequence of those inputs is to return an error that the method has not been implemented for psa. Hence, psa can only run with ‚Äúserial‚Äù or ‚Äúparallel‚Äù. Likewise for psaAboveDiagnosisThreshold().\nAt 14.16 (so having let the model run since 13.55, for 21 minutes), I cancelled the run on the remote computer, and altered the scenario script to set v0$method &lt;- \"parallel\". Then set that running, starting at 14.21. At this moment, the first scenario was still running on my machine (since 13.13, so for apx. 1h 10m now).\nAt 14.39, I noticed the parallel model said Killed. In case I might have accidentally done this, I just set it to rerun again. However, at 14.52, it showed as Killed again. Upon Googling, I can see this message occurs when you run out of memory - which perhaps, might be why all were set to run sequentially, if its too intensive to run in parallel. I switched it back to ‚Äúserial‚Äù.\nHowever, the model still running on my machine has now been going for 1h 42m. Having looked in the repository and article for any indication of runtime or requirement of HPC, I‚Äôve yet to find anything. I tried looking now at the article first describing the model (https://doi.org/10.1177/0272989X17753380). In this article, they state the run time:\nHowever, the computational requirements of the DES were extensive, given the number of individuals needed to reduce sampling variation to an acceptable level and characterizing uncertainty through PSA. Run time was in the region of 24 h to run the model with 500,000 patients and 1,000 PSA iterations, even with parallelization and the use of a high-powered computer.\nIn this case, ‚Äúeach scenario model is run for 10 million hypothetical individuals randomly drawn with replacement from the distribution of the relevant population. The models are run for a period of 30 years‚Äù. However, as there is no mention of performing probabilistic sensitivity analysis iterations in the article, we could expect this to be quicker than that (although note it is 10m patients instead of 500k).\nDiscussed with Tom and agreed to:\n\n\nTry and just run it with very few people, just so we can see if the model is working, and whether we could get similar results with fewer people in the model\n\n\nTry and run it sequentially on the remote computer overnight"
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#running-the-model-with-fewer-people",
    "href": "logbook/posts/2024_07_29/index.html#running-the-model-with-fewer-people",
    "title": "Day 2",
    "section": "15.09-15.42: Running the model with fewer people",
    "text": "15.09-15.42: Running the model with fewer people\nDES_Input_Definitions states that the number of people to run through the DES is defined by v0$numberOfPersons, default 1000. Can see in run_aaamodel_65yo_scen0.R that it has been set to v0$numberOfPersons &lt;- 1e7. I replaced this with v0$numberOfPersons &lt;- 1000. This finished almost immediately, then hit an error:\n&gt; TableOfCounts(scen0.invite, v1other)\nError in \"screen\" %in% eventHistory$events && !(\"nonvisualization\" %in%  : \n  'length = 3' in coercion to 'logical(1)'\nTableOfCounts() is from DES_Model.R. Working through each line of the function, I found that the error is occurring for scre_dropout=countDropouts(result, v1other).\nIn this function, personsInfo (result, ie. scen0.invite) is used. It loops through the EventHistories for each person: for (i in 1:length(personsInfo$eventHistories)). It checks whether their events:\n\nIncludes ‚Äúscreen‚Äù (\"screen\" %in% eventHistory$events)\nDoes not include ‚Äúnonvisualization‚Äù (!(\"nonvisualization\" %in% eventHistory$events))\nIf the measured size for screen is greater than or equal to v1other$aortaDiameterThresholds[[1]]\n\nThe error occurs for that final comparison. That is the item we had to change earlier to be able to get the model to run. I tried setting that back to being a list within a list - v1other$aortaDiameterThresholds &lt;- list(c(3.0, 4.5, 5.5)). However, I realised that caused the issue, and that in fact, run_aaamodel_65yo_scen0.R is changing it to list() again before running the model:\n## Change v1other$aortaDiameterThresholds to be a list (new syntax)\nv1other$aortaDiameterThresholds &lt;- list(v1other$aortaDiameterThresholds)\nIf I comment out that line, an error occurs. Hence, it seems we require it to be in a list within a list for the model, but just a list for TableOfCounts(). Looking at all the repository code, the reason becomes apparent in models/NAASP_COVID_modelling/run_aaamodel_surv_scen3.R, when multiple aaortaDiameterThresholds are provided.\nAs such, I set about modifying the later processing functions so that they are able to use this list() format:\n\ninput/NAAASP_Men_2020-05-11/DES_Data_Input_NAAASP_Men_30years_time_horizon_2020-05-11.R: returned back to list(c()), but then changed run_aaamodel_65yo_scen0.R to not add an additional list() over the top.\nDES_Model.R: countDropouts(): &gt;= v1other$aortaDiameterThresholds[[1]] to &gt;= v1other$aortaDiameterThresholds[[1]][1]\n\nThe whole script then ran successfully!\n\n\n\n\n\n\nReflection\n\n\n\nThese errors will likely be the result of having modified code but no re-run everything from scratch (unsurprising given the anticipated model run times)."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#inspecting-outcome-from-a-model-run",
    "href": "logbook/posts/2024_07_29/index.html#inspecting-outcome-from-a-model-run",
    "title": "Day 2",
    "section": "15.43-16.00, 16.03-16.43, 16.45-16.47, 16.53-16.56: Inspecting outcome from a model run",
    "text": "15.43-16.00, 16.03-16.43, 16.45-16.47, 16.53-16.56: Inspecting outcome from a model run\nThat R script ran the status quo (I0) scenario for invited 65 year olds. The parameters from Table 1 are below, along with their location in the data from DES_Input_Definitions.xlsx\n\nAttendance 75% - v2$probOfAttendScreen\nDrop-out rate/annum: 6% - v2$rateOfDropoutFromMonitoring\nThreshold for surgery: 5.5cm - v1other$aortaDiameterThresholds[[1]][3] (‚ÄúThese are the aortic cut-points where surveillance intervals change. Note, the first cut-point must always relate to the aortic size where individuals enter the AAA surveillance programme (e.g.¬†3.0cm). The last cut-point must always relate to the aortic size where surgery is to be considered (e.g.¬†5.5cm)‚Äù)\n\nLooking at the next R script, run_aaamodel_65yo_scen1.R, I can see that they repeat the model multiple times to get results from varying parameters of that scenario. That script is scenario 1 which, from Table 1, we can see delays invitation from 3 months to 5 years.\nI switched to trying that script, but again first setting:\n\n# v1other$aortaDiameterThresholds &lt;- list(v1other$aortaDiameterThresholds)\nv0$numberOfPersons &lt;- 1000\n\nThis ran eight ‚Äúscenarios‚Äù within the script. The output dataframe scen1summaryi showed the results from each of those eight variants. I add a line to the script to save this to csv. However, I could see that the number dead was 7 or 8 across all scenarios, so we likely did need to up the number in the model a bit more to start getting some real results.\n\nimport pandas as pd\n\nres = pd.read_csv('output_65yo_scen1_n1000.csv')\nres[['delayscr', 'aaadead']].sort_values(by='delayscr')\n\n\n\n\n\n\n\n\n\ndelayscr\naaadead\n\n\n\n\n7\n0.00\n7\n\n\n0\n0.25\n7\n\n\n1\n0.50\n7\n\n\n2\n1.00\n7\n\n\n3\n2.00\n7\n\n\n4\n3.00\n7\n\n\n5\n4.00\n8\n\n\n6\n5.00\n8\n\n\n\n\n\n\n\n\nI increased it to 10,000 patients. For each of the eight runs in the script, it took about 27 seconds (so under four minutes in total). Here we start to see more change between each scenario, and it becomes more feasible to look at the results.\n\nimport pandas as pd\n\nres = pd.read_csv('output_65yo_scen1_n10000.csv')\nres[['delayscr', 'aaadead', 'emeropen']].sort_values(by='delayscr')\n\n\n\n\n\n\n\n\n\ndelayscr\naaadead\nemeropen\n\n\n\n\n7\n0.00\n96\n33\n\n\n0\n0.25\n96\n33\n\n\n1\n0.50\n98\n32\n\n\n2\n1.00\n96\n31\n\n\n3\n2.00\n98\n31\n\n\n4\n3.00\n98\n32\n\n\n5\n4.00\n101\n32\n\n\n6\n5.00\n103\n32\n\n\n\n\n\n\n\n\nEach result here relates to Table 2 (i.e.¬†excess AAA deaths, and excess emergency operations). To calculate excess, given that the results appears to increment from the first 0 scenario, I‚Äôm assuming that this is the change in deaths from the 0 scenario.\nI created an .Rmd file to start processing the results. This required the addition of rmarkdown to the renv. I add dplyr for processing the results. Looking at excess deaths, we can see the anticipated pattern, although for emergency operations, the numbers are still too small that they fluctuate, and emergency operations goes in the wrong direction.\n\nimport pandas as pd\n\npd.read_csv('tab2.csv')\n\n\n\n\n\n\n\n\n\ndelayscr\nexcess_death\nexcess_op\n\n\n\n\n0\n0.00\n0\n0\n\n\n1\n0.25\n0\n0\n\n\n2\n0.50\n2\n-1\n\n\n3\n1.00\n0\n-2\n\n\n4\n2.00\n2\n-2\n\n\n5\n3.00\n2\n-1\n\n\n6\n4.00\n5\n-1\n\n\n7\n5.00\n7\n-1\n\n\n\n\n\n\n\n\nI tried upping it to 100,000 people, and recorded the time with different settings:\n\n‚Äúserial‚Äù - 5 minutes 7 seconds\n‚Äúparallel‚Äù - 2 minutes 12 seconds - so estimated 17 and a half minutes in total\n‚Äúforeach‚Äù - not possible, Error in processPersons(v0, v1other, v2, personData.screen) : v0$randomSeed does not yet work with v0$method=\"foreach\"\n\nFor each of these, about 45 seconds is results processing at the end (e.g.¬†Eventsandcosts())\nWhilst these ran, I looked through the repository to try and spot whether they had functions to generate the plots from the article. I found plotting functions in DES_Model.R, shiny_output_functions.R and NAAASP_DES_Example_Script.R, though none relevant to the article, so it appears I‚Äôll need to write the code to do that processing."
  },
  {
    "objectID": "logbook/posts/2024_07_29/index.html#timings",
    "href": "logbook/posts/2024_07_29/index.html#timings",
    "title": "Day 2",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 32\n\n# Times from today\ntimes = [\n    ('09.38', '09.43'),\n    ('09.53', '09.55'),\n    ('10.05', '10.15'),\n    ('10.29', '10.45'),\n    ('10.46', '11.02'),\n    ('11.09', '11.13'),\n    ('11.26', '11.32'),\n    ('12.50', '12.56'),\n    ('12.57', '13.04'),\n    ('13.05', '13.13'), \n    ('13.48', '13.55'),\n    ('14.07', '14.21'),\n    ('14.53', '15.08'),\n    ('15.09', '15.42'),\n    ('15.43', '16.00'),\n    ('16.03', '16.43'),\n    ('16.45', '16.47'),\n    ('16.53', '16.56')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 211m, or 3h 31m\nTotal used to date: 243m, or 4h 3m\nTime remaining: 2157m, or 35h 57m\nUsed 10.1% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html",
    "href": "logbook/posts/2024_08_02/index.html",
    "title": "Day 6",
    "section": "",
    "text": "Note\n\n\n\nFinished reproduction (supplementary table 2 and figure). Completed evaluation against guidelines. Started on research compendium. Total reproduction time: 13h 59m (35.0%). Evaluation time: 1h 55m."
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#resuming-supplementary-table-2",
    "href": "logbook/posts/2024_08_02/index.html#resuming-supplementary-table-2",
    "title": "Day 6",
    "section": "09.09-09.43: Resuming supplementary table 2",
    "text": "09.09-09.43: Resuming supplementary table 2\nCombined each scenario with scenario 0, then adapted previous functions to generate the table.\nConsider this reproduced at 09.43.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 805\n\n# Times from today\ntimes = [\n    ('09.09', '09.43')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 34m, or 0h 34m\nTotal used to date: 839m, or 13h 59m\nTime remaining: 1561m, or 26h 1m\nUsed 35.0% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#supplementary-figure-3",
    "href": "logbook/posts/2024_08_02/index.html#supplementary-figure-3",
    "title": "Day 6",
    "section": "10.02-10.38: Supplementary figure 3",
    "text": "10.02-10.38: Supplementary figure 3\nAmend prior functions to combine with scenario 0 using a function (as repeated lots, and need again for this figure). Could then use those dataframes in my figure functions.\nConsider this reproduced at 10.38."
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#reproduction-timings",
    "href": "logbook/posts/2024_08_02/index.html#reproduction-timings",
    "title": "Day 6",
    "section": "Reproduction timings",
    "text": "Reproduction timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 805\n\n# Times from today\ntimes = [\n    ('09.09', '09.43'),\n    ('10.02', '10.38')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 70m, or 1h 10m\nTotal used to date: 875m, or 14h 35m\nTime remaining: 1525m, or 25h 25m\nUsed 36.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#badges",
    "href": "logbook/posts/2024_08_02/index.html#badges",
    "title": "Day 6",
    "section": "10.54-10.59: Badges",
    "text": "10.54-10.59: Badges\nEvaluated repository as available at this time (so including the few commits since publication).\ndocumentation_sufficient - no as no package versions and as most instructions are within R scripts themselves (e.g.¬†that need to run with source()) (and therefore not met the subsequent careful documentation)\ndocumentation_readme - although it directs to a good example file, it doesn‚Äôt include instructions for reproducing results from the article"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#stars-framework",
    "href": "logbook/posts/2024_08_02/index.html#stars-framework",
    "title": "Day 6",
    "section": "11.00-11.05, 11.18-11.23, 11.29-11.32: STARS framework",
    "text": "11.00-11.05, 11.18-11.23, 11.29-11.32: STARS framework"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#stress-des",
    "href": "logbook/posts/2024_08_02/index.html#stress-des",
    "title": "Day 6",
    "section": "11.33-12.10, 12.55-13.10, 13.15-13.30: STRESS-DES",
    "text": "11.33-12.10, 12.55-13.10, 13.15-13.30: STRESS-DES\nTo discuss (combined with ISPOR-SDM when overlap):\n\n2.1 Base model overview diagram / 3 Is the model structure described? - diagram is in prior study\n2.2 Base model logic - described in detail in prior study, only briefly alluded to in this one\n2.4 Algorithms - partially described in prior study\n2.5.2 Components - activities - provided moreso in prior study\n2.5.3 Components - resources - not clear in either study, unlesss there are none?\n2.5.4 Components - queues - not clear in either study, unless there are none?\n4.1 Initialisation - indicates its non-terminating in prior study\n5.3 Model execution - prior study mentions that you can run in parallel\n5.4 System specification - prior study mentions HPC"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#ispor-sdm",
    "href": "logbook/posts/2024_08_02/index.html#ispor-sdm",
    "title": "Day 6",
    "section": "13.48-14.18: ISPOR-SDM",
    "text": "13.48-14.18: ISPOR-SDM"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#evaluation-timings",
    "href": "logbook/posts/2024_08_02/index.html#evaluation-timings",
    "title": "Day 6",
    "section": "Evaluation timings",
    "text": "Evaluation timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('10.54', '10.59'),\n    ('11.00', '11.05'),\n    ('11.18', '11.23'),\n    ('11.29', '11.32'),\n    ('11.33', '12.10'),\n    ('12.55', '13.10'),\n    ('13.15', '13.30'),\n    ('13.48', '14.18')]\n\ncalculate_times(used_to_date, times, limit=False)\n\nTime spent today: 115m, or 1h 55m\nTotal used to date: 115m, or 1h 55m"
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#untimed-summary-report",
    "href": "logbook/posts/2024_08_02/index.html#untimed-summary-report",
    "title": "Day 6",
    "section": "Untimed: Summary report",
    "text": "Untimed: Summary report\nPartially completed summary report (evaluation will need to be updated following second opinion, as there were several items I marked as TBC, as I wasn‚Äôt certain whether they met criteria when the information was from a prior study)."
  },
  {
    "objectID": "logbook/posts/2024_08_02/index.html#untimed-reflections",
    "href": "logbook/posts/2024_08_02/index.html#untimed-reflections",
    "title": "Day 6",
    "section": "Untimed: Reflections",
    "text": "Untimed: Reflections\nDetailed troubleshooting steps and add reflections."
  },
  {
    "objectID": "logbook/logbook.html",
    "href": "logbook/logbook.html",
    "title": "Logbook",
    "section": "",
    "text": "These diary entries record daily progress in reproduction of the study, providing a transparent and detailed record of work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 7\n\n\n\n\n\n\nreport\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nAug 12, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 6\n\n\n\n\n\n\nreproduce\n\n\nguidelines\n\n\nreport\n\n\nreflections\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\nread\n\n\nscope\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\n\n\n\nsetup\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nAmy Heather\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "evaluation/reflections.html",
    "href": "evaluation/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "This page contains reflections on the facilitators and barriers to this reproduction, as well as a full list of the troubleshooting steps taken to reproduce this work."
  },
  {
    "objectID": "evaluation/reflections.html#what-helped-facilitate-this-reproduction",
    "href": "evaluation/reflections.html#what-helped-facilitate-this-reproduction",
    "title": "Reflections",
    "section": "What helped facilitate this reproduction?",
    "text": "What helped facilitate this reproduction?\n\nProvides commands to install packages required at the start of scripts, which I could then easily base renv on automatically (as it detects them)\nUsed seeds to control model (so got consistent results between runs with matching parameters)\nProvides the option of running with parallel processing\nRelatively simple and repetitive plots which are easy to understand and create"
  },
  {
    "objectID": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "href": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "title": "Reflections",
    "section": "What would have helped facilitate this reproduction?",
    "text": "What would have helped facilitate this reproduction?\nProvide environment (packages and versions)\n\nThere were no issues using the latest environment, but if there had been, it would be important to know what versions had previously been used and worked\n\nRun time\n\nMaking note of the high run time in the paper and repository, and providing suggestions on how to run the model quicker or on a lower-spec machine (i.e.¬†not on a high-performance computer) - for example, running in parallel, and suggesting a lower numebr of people in simulation\n\nFix error in script\n\nAn error appears to have been introduced with the aoorta diameter thresholds by switching between nested and unnested lists, which I‚Äôm anticipating was unresolved due to the long run times of the model meaning they weren‚Äôt all run in sequence at the end.\n\nProvide code to implement scenarios and generate outputs\n\nInclude code to find the aorta sizes of people with AAA-related deaths\nProvide code to produce the tables\nProvide code to produce the figures"
  },
  {
    "objectID": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "href": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "title": "Reflections",
    "section": "Full list of troubleshooting steps",
    "text": "Full list of troubleshooting steps\n\n\n\n\n\n\nView list\n\n\n\n\n\nTroubleshooting steps are grouped by theme, and the day these occurred is given in brackets at the end of each bullet.\n\nEnvironment\n\nNo environment but does state version of R (3.6.3) and has commands to install required packages (no specific versions). Created an renv with latest versions and then had no issues with environment. (2)\n\n\n\nRunning the scripts\n\nAfter a failed attempt, realised that need to run scripts using source() (and this is mentioned at the start of run_aaamodel_65yo_scen0.R). (2)\nDid not realise the high run time of these models and how computationally expensive they were until I tried running them (and later realising its mentioned in the prior study, Glover et al.¬†2018). Had to:\n\nExperiment with different numbers of people in the simulation to identify a number that was high enough to get reasonably similar results to the original study, but low enough to be feasible to run on our machines. (2)\nSwitch from serial to parallel (this was provided in the script already, just had to change the parameter) (2)\n\n\n\n\nFixing errors in scripts\n\nThe aaorta diameter thresholds need to be provided as a nested list for some aspects of the script but a normal list for others. This was not implemented in the script, so had to:\n\nRemoved addition of list() in the run script (2)\nChange countDropouts() to get first element of list (i.e.¬†to unnest it) (2)\n\n\n\n\nWriting code to generate outputs\n\nHad to write code to generate tables‚Ä¶\n\nCalculate excess deaths and emergency operations (3+)\nScale to population size (3+)\nIdentify which columns provide the operation outcomes (need to combine two) (3)\n\nHad to write code to produce figures\n\nCalculate percentage change in outcomes (3+)\nPlot figures (3+)\n\n\n\n\nWriting code to find deaths by aorta size\n\nAdd code to the model scripts that will save the aorta sizes of people with AAA-related deaths, used to produce in-text result 1 (4-5)"
  },
  {
    "objectID": "evaluation/badges.html",
    "href": "evaluation/badges.html",
    "title": "Journal badges",
    "section": "",
    "text": "This page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.\nCaveat: Please note that these criteria are based on available information about each badge online, and that we have likely differences in our procedure (e.g.¬†allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals."
  },
  {
    "objectID": "evaluation/badges.html#criteria",
    "href": "evaluation/badges.html#criteria",
    "title": "Journal badges",
    "section": "Criteria",
    "text": "Criteria\n\n\nCode\nfrom IPython.display import display, Markdown\nimport numpy as np\nimport pandas as pd\n\n# Criteria and their definitions\ncriteria = {\n    'archive': 'Stored in a permanent archive that is publicly and openly accessible',\n    'id': 'Has a persistent identifier',\n    'license': 'Includes an open license',\n    'relevant': '''Artefacts are relevant to and contribute to the article's results''',\n    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',\n    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)',\n    'documentation_sufficient': 'Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)',\n    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)',\n    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file\n    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript',\n    'execute': 'Scripts can be successfully executed',\n    'regenerated': 'Independent party regenerated results using the authors research artefacts',\n    'hour': 'Reproduced within approximately one hour (excluding compute time)',\n}\n\n# Evaluation for this study\n# TODO: Complete evaluate for each criteria\neval = pd.Series({\n    'archive': 0,\n    'id': 0,\n    'license': 1,\n    'relevant': 1,\n    'complete': 0,\n    'structure': 1,\n    'documentation_sufficient': 0,\n    'documentation_careful': 0,\n    'documentation_readme': 0,\n    'execute': 1,\n    'regenerated': 1,\n    'hour': 0,\n})\n\n# Get list of criteria met (True/False) overall\neval_list = list(eval)\n\n# Define function for creating the markdown formatted list of criteria met\ndef create_criteria_list(criteria_dict):\n    '''\n    Creates a string which contains a Markdown formatted list with icons to\n    indicate whether each criteria was met\n\n    Parameters:\n    -----------\n    criteria_dict : dict\n        Dictionary where keys are the criteria (variable name) and values are\n        Boolean (True/False of whether this study met the criteria)\n\n    Returns:\n    --------\n    formatted_list : string\n        Markdown formatted list\n    '''\n    callout_icon = {True: '‚úÖ',\n                    False: '‚ùå'}\n    # Create list with...\n    formatted_list = ''.join([\n        '* ' +\n        callout_icon[eval[key]] + # Icon based on whether it met criteria\n        ' ' +\n        value + # Full text description of criteria\n        '\\n' for key, value in criteria_dict.items()])\n    return(formatted_list)\n\n# Define groups of criteria\ncriteria_share_how = ['archive', 'id', 'license']\ncriteria_share_what = ['relevant', 'complete']\ncriteria_doc_struc = ['structure', 'documentation_sufficient', 'documentation_careful', 'documentation_readme']\ncriteria_run = ['execute', 'regenerated', 'hour']\n\n# Create text section\ndisplay(Markdown(f'''\nTo assess whether the author's materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\n\nThis study met **{sum(eval_list)} of the {len(eval_list)}** unique criteria items. These were as follows:\n\nCriteria related to how artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_how})}\n\nCriteria related to what artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_what})}\n\nCriteria related to the structure and documentation of the artefacts -\n\n{create_criteria_list({k: criteria[k] for k in criteria_doc_struc})}\n\nCriteria related to running and reproducing results -\n\n{create_criteria_list({k: criteria[k] for k in criteria_run})}\n'''))\n\n\nTo assess whether the author‚Äôs materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\nThis study met 5 of the 12 unique criteria items. These were as follows:\nCriteria related to how artefacts are shared -\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n\nCriteria related to what artefacts are shared -\n\n‚úÖ Artefacts are relevant to and contribute to the article‚Äôs results\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n\nCriteria related to the structure and documentation of the artefacts -\n\n‚úÖ Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚ùå Artefacts are carefully documented (more than sufficient - i.e.¬†to the extent that reuse and repurposing is facilitated - e.g.¬†changing parameters, reusing for own purpose)\n‚ùå Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n\nCriteria related to running and reproducing results -\n\n‚úÖ Scripts can be successfully executed\n‚úÖ Independent party regenerated results using the authors research artefacts\n‚ùå Reproduced within approximately one hour (excluding compute time)"
  },
  {
    "objectID": "evaluation/badges.html#badges",
    "href": "evaluation/badges.html#badges",
    "title": "Journal badges",
    "section": "Badges",
    "text": "Badges\n\n\nCode\n# Full badge names\nbadge_names = {\n    # Open objects\n    'open_niso': 'NISO \"Open Research Objects (ORO)\"',\n    'open_niso_all': 'NISO \"Open Research Objects - All (ORO-A)\"',\n    'open_acm': 'ACM \"Artifacts Available\"',\n    'open_cos': 'COS \"Open Code\"',\n    'open_ieee': 'IEEE \"Code Available\"',\n    # Object review\n    'review_acm_functional': 'ACM \"Artifacts Evaluated - Functional\"',\n    'review_acm_reusable': 'ACM \"Artifacts Evaluated - Reusable\"',\n    'review_ieee': 'IEEE \"Code Reviewed\"',\n    # Results reproduced\n    'reproduce_niso': 'NISO \"Results Reproduced (ROR-R)\"',\n    'reproduce_acm': 'ACM \"Results Reproduced\"',\n    'reproduce_ieee': 'IEEE \"Code Reproducible\"',\n    'reproduce_psy': 'Psychological Science \"Computational Reproducibility\"'\n}\n\n# Criteria required by each badge\nbadges = {\n    # Open objects\n    'open_niso': ['archive', 'id', 'license'],\n    'open_niso_all': ['archive', 'id', 'license', 'complete'],\n    'open_acm': ['archive', 'id'],\n    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],\n    'open_ieee': ['complete'],\n    # Object review\n    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],\n    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],\n    'review_ieee': ['complete', 'execute'],\n    # Results reproduced\n    'reproduce_niso': ['regenerated'],\n    'reproduce_acm': ['regenerated'],\n    'reproduce_ieee': ['regenerated'],\n    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],\n}\n\n# Identify which badges would be awarded based on criteria\n# Get list of badges met (True/False) overall\naward = {}\nfor badge in badges:\n    award[badge] = all([eval[key] == 1 for key in badges[badge]])\naward_list = list(award.values())\n\n# Write introduction\n# Get list of badges met (True/False) by category\naward_open = [v for k,v in award.items() if k.startswith('open_')]\naward_review = [v for k,v in award.items() if k.startswith('review_')]\naward_reproduce = [v for k,v in award.items() if k.startswith('reproduce_')]\n\n# Create and display text for introduction\ndisplay(Markdown(f'''\nIn total, the original study met the criteria for **{sum(award_list)} of the {len(award_list)} badges**. This included:\n\n* **{sum(award_open)} of the {len(award_open)}** ‚Äúopen objects‚Äù badges\n* **{sum(award_review)} of the {len(award_review)}** ‚Äúobject review‚Äù badges\n* **{sum(award_reproduce)} of the {len(award_reproduce)}** ‚Äúreproduced‚Äù badges\n'''))\n\n# Make function that creates collapsible callouts for each badge\ndef create_badge_callout(award_dict):\n    '''\n    Displays Markdown callouts created for each badge in the dictionary, showing\n    whether the criteria for that badge was met.\n\n    Parameters:\n    -----------\n    award_dict : dict\n        Dictionary where key is badge (as variable name), and value is Boolean\n        (whether badge is awarded)\n    '''\n    callout_appearance = {True: 'tip',\n                          False: 'warning'}\n    callout_icon = {True: '‚úÖ',\n                    False: '‚ùå'}\n    callout_text = {True: 'Meets all criteria:',\n                    False: 'Does not meet all criteria:'}\n\n    for key, value in award_dict.items():\n        # Create Markdown list with...\n        criteria_list = ''.join([\n            '* ' +\n            callout_icon[eval[k]] + # Icon based on whether it met criteria\n            ' ' +\n            criteria[k] + # Full text description of criteria\n            '\\n' for k in badges[key]])\n        # Create the callout and display it\n        display(Markdown(f'''\n::: {{.callout-{callout_appearance[value]} appearance=\"minimal\" collapse=true}}\n\n## {callout_icon[value]} {badge_names[key]}\n\n{callout_text[value]}\n\n{criteria_list}\n:::\n'''))\n\n# Create badge functions with introductions and callouts\ndisplay(Markdown('''\n### \"Open objects\" badges\n\nThese badges relate to research artefacts being made openly available.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('open_')})\n\ndisplay(Markdown('''\n### \"Object review\" badges\n\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('review_')})\n\ndisplay(Markdown('''\n### \"Reproduced\" badges\n\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('reproduce_')})\n\n\nIn total, the original study met the criteria for 3 of the 12 badges. This included:\n\n0 of the 5 ‚Äúopen objects‚Äù badges\n0 of the 3 ‚Äúobject review‚Äù badges\n3 of the 4 ‚Äúreproduced‚Äù badges\n\n\n\n‚ÄúOpen objects‚Äù badges\nThese badges relate to research artefacts being made openly available.\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúOpen Research Objects (ORO)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúOpen Research Objects - All (ORO-A)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Available‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå COS ‚ÄúOpen Code‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå IEEE ‚ÄúCode Available‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n‚ÄúObject review‚Äù badges\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Evaluated - Functional‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚úÖ Artefacts are relevant to and contribute to the article‚Äôs results\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Evaluated - Reusable‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚ùå Artefacts are carefully documented (more than sufficient - i.e.¬†to the extent that reuse and repurposing is facilitated - e.g.¬†changing parameters, reusing for own purpose)\n‚úÖ Artefacts are relevant to and contribute to the article‚Äôs results\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n‚úÖ Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå IEEE ‚ÄúCode Reviewed‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n\n\n\n\n\n\n‚ÄúReproduced‚Äù badges\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n‚úÖ NISO ‚ÄúResults Reproduced (ROR-R)‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ ACM ‚ÄúResults Reproduced‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ IEEE ‚ÄúCode Reproducible‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå Psychological Science ‚ÄúComputational Reproducibility‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n‚ùå Reproduced within approximately one hour (excluding compute time)\n‚úÖ Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n‚ùå Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript"
  },
  {
    "objectID": "evaluation/badges.html#sources",
    "href": "evaluation/badges.html#sources",
    "title": "Journal badges",
    "section": "Sources",
    "text": "Sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n‚ÄúOpen Research Objects (ORO)‚Äù\n‚ÄúOpen Research Objects - All (ORO-A)‚Äù\n‚ÄúResults Reproduced (ROR-R)‚Äù\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n‚ÄúArtifacts Available‚Äù\n‚ÄúArtifacts Evaluated - Functional‚Äù\n‚ÄúArtifacts Evaluated - Resuable‚Äù\n‚ÄúResults Reproduced‚Äù\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n‚ÄúOpen Code‚Äù\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\n\n‚ÄúCode Available‚Äù\n‚ÄúCode Reviewed‚Äù\n‚ÄúCode Reproducible‚Äù\n\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\n‚ÄúComputational Reproducibility‚Äù"
  },
  {
    "objectID": "evaluation/reporting.html",
    "href": "evaluation/reporting.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "This page evaluates the extent to which the journal article meets the criteria from two discrete-event simulation study reporting guidelines:"
  },
  {
    "objectID": "evaluation/reporting.html#stress-des",
    "href": "evaluation/reporting.html#stress-des",
    "title": "Reporting guidelines",
    "section": "STRESS-DES",
    "text": "STRESS-DES\nOf the 24 items in the checklist:\n\n\nX were met fully (‚úÖ)\nX were partially met (üü°)\nX were not met (‚ùå)\nX were not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\n\nItem\nRecommendation\nMet by study?\nEvidence\n\n\n\n\nObjectives\n\n\n\n\n\n1.1 Purpose of the model\nExplain the background and objectives for the model\n‚úÖ Fully\nIntroduction - Abdominal Aortic Aneurysm (AAA) ‚Äúscreening (including surveillance) in most areas of the UK was paused during the lockdown due to concerns about COVID-19 transmission‚Äù‚Ä¶ this study uses a previously developed and validated model ‚Äúto explore different approaches to post-lockdown service resumption‚ÄùKim et al. (2021)\n\n\n1.2 Model outputs\nDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n‚úÖ Fully\nMethods: Post-COVID-19 policy scenarios - ‚ÄúTotal numbers of AAA- related deaths, operations (both elective and emergency) and ruptures over the whole follow- up period are recorded for each model. These clinical results are reported as percentage change from the status quo as well as expected increase in number of events when scaled to the popu- lation of England‚ÄùKim et al. (2021)\n\n\n1.3 Experimentation aims\nIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis ‚Äì Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments ‚Äì Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation ‚Äì (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n‚úÖ Fully\nPrimary scenarios clearly described in Table 1.Supplementary scenarios not described as clearly, but can be easily understood from supplementary table 2, regardless.Kim et al. (2021)\n\n\nLogic\n\n\n\n\n\n2.1 Base model overview diagram\nDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\nTBC\nNot provided in this study, but states in Methods: Model that this is a model that ‚Äúhas previously been developed‚Äù and ‚Äúthe full pathway reflecting both the natural history and screening programme is described in detailed elsewhere‚Äù. It cites Glover et al.¬†2018 and Thompson et al.¬†2018. Glover et al.¬†2018 includes a flow diagram (Figure 1).\n\n\n2.2 Base model logic\nGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\nTBC\nMethods: Model does mention that this is described elsewhere, but ‚Äúin brief‚Ä¶ the original DES model simulated events for a new cohort at a given age (e.g.¬†65) from the time of invitation to screening up to their date of death or age 95 (the time horizon). The repurposed DES model is extended here to allow events to be simulated from a cohort of individuals already under surveillance in the NAAASP, through simulation of key characteristics (age and aortic diameter) at the inception of the model (‚Äútime zero‚Äù), which is taken to be March 2020 when the initial UK national ‚Äúlockdown‚Äù was imposed‚Äù. It cites Glover et al.¬†2018 which provides a very detailed description of the base model logic.Kim et al. (2021)\n\n\n2.3 Scenario logic\nGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n‚úÖ Fully\nDescribed in the text, and main scenarios clearly presented in Table 1.Kim et al. (2021)\n\n\n2.4 Algorithms\nProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e.¬†scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\nTBC\nNot provided in this study, but states in Methods: Model that this is a model that has previously been developed, with the cited Glover et al.¬†2018 providing some (but not all) of the sampling distributions used‚Ä¶ ‚ÄúThe DES involves a large number of parameters. These can be classified into several sets: global fixed parameters, global uncertain parameters, and parameters that are specific to an individual or a pair of twins (‚Äúglobal‚Äù refers to population parameters and ‚Äúuncertain‚Äù means that a parameter follows a random distribution). Like the functions, these sets form a hierarchy. For example, in a PSA, a beta distribution is used to generate the probability that an individual will die following emergency surgery, if they have emergency surgery. The parameters of the beta distribution are global fixed parameters, and the probability is a global uncertain parameter. In the main analysis, when a pair of twins is created, the probability is used as the parameter in a Bernoulli distribution to generate the indicator for the twins‚Äô emergency surgery outcomes. The indicator is a variable specific to the pair of twins.‚ÄùGlover et al. (2018)\n\n\n2.5.1 Components - entities\nGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n‚úÖ Fully\nMethods: Model - ‚ÄúThe original DES model simulated events for a new cohort at a given age (e.g.¬†65) from the time of invitation to screening up to their date of death or age 95 (the time horizon). The repurposed DES model is extended here to allow events to be simulated from a cohort of indi- viduals already under surveillance in the NAAASP, through simulation of key characteristics (age and aortic diameter) at the inception of the model (‚Äútime zero‚Äù), which is taken to be March 2020 when the initial UK national ‚Äúlockdown‚Äù was imposed.‚ÄùKim et al. (2021)\n\n\n2.5.2 Components - activities\nDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\nTBC\nNot described in this paper, but cites Glover et al. (2018) as providing further details. In their paper, they detail activities in the Methods or show them within Figure 1.\n\n\n2.5.3 Components - resources\nList all the resources included within the model and which activities make use of them.\nTBC\n-\n\n\n2.5.4 Components - queues\nGive details of the assumed queuing discipline used in the model (e.g.¬†First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\nTBC\n-\n\n\n2.5.5 Components - entry/exit points\nGive details of the model boundaries i.e.¬†all arrival and exit points of entities. Detail the arrival mechanism (e.g.¬†‚Äòthinning‚Äô to mimic a non-homogenous Poisson process or balking)\n‚úÖ Fully\nMethods: Model - entities are followed ‚Äúat a given age (e.g.¬†65) from the time of invitation to screening up to their date of death or age 95 (time horizon)‚ÄùKim et al. (2021)\n\n\nData\n\n\n\n\n\n3.1 Data sources\nList and detail all data sources. Sources may include:‚Ä¢ Interviews with stakeholders,‚Ä¢ Samples of routinely collected data,‚Ä¢ Prospectively collected samples for the purpose of the simulation study,‚Ä¢ Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n‚úÖ Fully\nProvided for each input in Supplementary Table 1\n\n\n3.2 Pre-processing\nProvide details of any data manipulation that has taken place before its use in the simulation, e.g.¬†interpolation to account for missing data or the removal of outliers.\n‚úÖ Fully\nSome processing is described in Supplementary Table 1\n\n\n3.3 Input parameters\nList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:‚Ä¢ Base case data‚Ä¢ Data use in experimentation, where different from the base case.‚Ä¢ Where optimisation or design of experiments has been used, state the range of values that parameters can take.‚Ä¢ Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\nüü° Partially\nInputs provided in Supplementary Table 1. Distributions are not provided. In the seminal paper for this model (Glover et al. (2018)), they describe some but not all of the distributions.\n\n\n3.4 Assumptions\nWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n‚úÖ Fully\nHas a section addressing this - Discussion: Modelling assumptions\n\n\nExperimentation\n\n\n\n\n\n4.1 Initialisation\nReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\nTBC\nNot mentioned in this paper, but prior paper mentions that events ‚Äúoccur as a continuous process over time‚Äù (i.e.¬†non-terminating). Glover et al. (2018). I did not feel I could clearly identify, from either paper, whether a warm-up or initialisation conditions were used.\n\n\n4.2 Run length\nDetail the run length of the simulation model and time units.\n‚úÖ Fully\nMethods: Post-COVID-19 policy scenarios - ‚ÄúTHe models are run for a period of 30 years‚ÄùKim et al. (2021)\n\n\n4.3 Estimation approach\nState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n‚úÖ Fully\nMethods: Post-COVID-19 policy scenarios - ‚ÄúEach scenario model is run for 10 million hypothetical individuals‚Ä¶ Model convergence is summarised using cumulative results from consecutive sub-runs each of 1 million individuals‚Äù (Supplementary Figure 1, Supplementary Figure 2)\n\n\nImplementation\n\n\n\n\n\n5.1 Software or programming language\nState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g.¬†Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\nüü° Partially\nMentions that it uses R and a version. Doesn‚Äôt mention packages or their versions.Methods: Model - ‚Äúmodel has previously been developed in R version 3.6.3‚ÄùKim et al. (2021)\n\n\n5.2 Random sampling\nState the algorithm used to generate random samples in the software/programming language used e.g.¬†Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n‚ùå Not met\nCouldn‚Äôt spot in Kim et al.¬†2021 (or in Glover et al.¬†2018).\n\n\n5.3 Model execution\nState the event processing mechanism used e.g.¬†three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\nTBC\nDidn‚Äôt feel I could find the event processing mechanism. The prior study does mention though that the code is ‚Äúwritten to run in parallel‚Äù.Glover et al. (2018)\n\n\n5.4 System specification\nState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\nTBC\nNot mentioned in this study, but prior study mentions that ‚Äúrun time was in the region of 24 h to run the model with 500,000 patients and 1,000 PSA iterations, even with parallelization and the use of a high-powered compute‚Äù.Glover et al. (2018)\n\n\nCode access\n\n\n\n\n\n6.1 Computer model sharing statement\nDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n‚úÖ Fully\nData Availability Statement: ‚ÄúThe DES model used in this work is available on a GitHub repository https://github.com/mikesweeting/AAA_DES_model‚Äù"
  },
  {
    "objectID": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "href": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "title": "Reporting guidelines",
    "section": "DES checklist derived from ISPOR-SDM",
    "text": "DES checklist derived from ISPOR-SDM\nOf the 18 items in the checklist:\n\n\nX were met fully (‚úÖ)\nX were partially met (üü°)\nX were not met (‚ùå)\nX were not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\nItem\nAssessed if‚Ä¶\nMet by study?\nEvidence/location\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?\n‚Ä¶the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n‚úÖ Fully\nIntroduction - explained in detail - e.g.¬†COVID-19 disrupted ‚Äúrepair of Abdominal Aortic Aneurysms (AAA). Furthermore, AAA screening (including surveillance) in most areas of the UK was paused during the lockdown due to concerns about COVID-19 transmission‚Ä¶ Ruptured AAA carries a high mortality and screening for AAA is offered to men in their 65th year throughout England via the NHS Abdominal Aortic Aneurysm Screening Program (NAAASP)‚Ä¶‚Äù. This explores ‚Äúdifferent approaches to post-lockdown service resumption‚Äù\n\n\n2 Is the modeled healthcare setting/health condition clarified?\n‚Ä¶the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n‚úÖ Fully\nIntroduction - nationwide, e.g.¬†‚ÄúNHS Abdominal Aortic Aneurysm Screening Porgram (NAASP)‚ÄùKim et al. (2021)\n\n\n3 Is the model structure described?\n‚Ä¶the model‚Äôs conceptual structure was described in the form of either graphical or text presentation.\nTBC\nNot provided in this study, but states in Methods: Model that this is a model that ‚Äúhas previously been developed‚Äù and ‚Äúthe full pathway reflecting both the natural history and screening programme is described in detailed elsewhere‚Äù. It cites Glover et al.¬†2018 and Thompson et al.¬†2018. Glover et al.¬†2018 includes a flow diagram (Figure 1).\n\n\n4 Is the time horizon given?\n‚Ä¶the time period covered by the simulation was reported.\n‚úÖ Fully\nMethods: Post-COVID-19 policy scenarios - ‚ÄúThe models are run for a period of 30 years‚ÄùKim et al. (2021)\n\n\n5 Are all simulated strategies/scenarios specified?\n‚Ä¶the comparators under test were described in terms of their components, corresponding variations, etc\n‚úÖ Fully\nPrimary scenarios clearly described in Table 1.Supplementary scenarios not described as clearly, but can be easily understood from supplementary table 2, regardless.Kim et al. (2021)\n\n\n6 Is the target population described?\n‚Ä¶the entities simulated and their main attributes were characterized.\n‚úÖ Fully\nIntroduction - e.g.¬†‚ÄúCirca 300,000 men are offered screening annually, of whom around 1% are found to have an AAA, whilst approximately 15,000 men are currently under surveillance in the programme.‚ÄùKim et al. (2021)\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?\n‚Ä¶the sources of all data used to inform model inputs were reported.\n‚úÖ Fully\nProvided for each input in Supplementary Table 1\n\n\n8 Are the parameters used to populate model frameworks specified?\n‚Ä¶all relevant parameters fed into model frameworks were disclosed.\n‚úÖ Fully\nInputs provided in Supplementary Table 1.\n\n\n9 Are model uncertainties discussed?\n‚Ä¶the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n‚ùå Not met\nNo - only presents counts or percentage change.\n\n\n10 Are sensitivity analyses performed and reported?\n‚Ä¶the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters‚Äô plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n‚ùå Not met\nNone mentioned in paper.\n\n\nValidation\n\n\n\n\n\n11 Is face validity evaluated and reported?\n‚Ä¶it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n‚ùå Not met\nI couldn‚Äôt spot any in this paper or the prior study.\n\n\n12 Is cross validation performed and reported\n‚Ä¶comparison across similar modeling studies which deal with the same decision problem was undertaken.\n‚úÖ Fully\nDiscussion: Strengths and limitations - ‚ÄúDES has been well validated against‚Ä¶ a previous Markov model, producing reliable estimates of events over the trial follow-up.‚ÄùIn the discussion, they compare against a ‚Äúrecent modelling study in the United States [that] explored the trade-off between COVID-19 mortality and AAA-related mortality‚ÄùKim et al. (2021)\n\n\n13 Is external validation performed and reported?\n‚Ä¶the modeler(s) examined how well the model‚Äôs results match the empirical data of an actual event modeled.\n‚úÖ Fully\nDiscussion: Strengths and limitations - ‚ÄúDES has been well validated against data from the Multicentre Aneurysm Screening Study‚ÄùKim et al. (2021)\n\n\n14 Is predictive validation performed or attempted?\n‚Ä¶the modeler(s) examined the consistency of a model‚Äôs predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\nN/A\nOnly relevant to forecasting studies.\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n15 Is the model generalizability issue discussed?\n‚Ä¶the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n‚ùå Not met\nCouldn‚Äôt spot anything in the paper.\n\n\n16 Are decision makers or other stakeholders involved in modeling?\n‚Ä¶the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n‚ùå Not met\nCouldn‚Äôt spot anything in this paper or the prior study.\n\n\n17 Is the source of funding stated?\n‚Ä¶the sponsorship of the study was indicated.\n‚úÖ Fully\n‚ÄúThis work was supported by core funding from: the UK Medical Research Council (MR/L003120/1), the British Heart Foundation (RG/13/13/30194) and the NIHR Cambridge Biomedical Research Centre (BRC) [The views expressed are those of the author(s) and not necessarily those of the NIHR or the Department of Health and Social Care]. LGK is funded by the NIHR Blood and Transplant Research Unit in Donor Health and Genomics (NIHR BTRU-2014-10024). SCH is funded by an MRC CARP Fellowship (Mr/T023783/1). https://mrc.ukri.org/ https://www.bhf.org.uk/for-professionals https://cambridgebrc.nihr.ac.uk/ http://www.donorhealth-btru.nihr.ac.uk/. The sponsors and funders played no role in study design. data collection, decision to publish or preparation of the manuscript.‚ÄùKim et al. (2021)\n\n\n18 Are model limitations discussed?\n‚Ä¶limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n‚úÖ Fully\nDiscussion: Strengths and limitations - e.g.¬†‚ÄúThere are a number of simplifications relating to model structure that were necessary for carrying out this COVID-19-related modelling work‚Ä¶ In addition to these structural assumptions, there are also challenges associated with extrapolating the underlying models of AAA growth and rupture rates to this setting‚Ä¶‚Äù"
  },
  {
    "objectID": "reproduction/process_results/process_results.html",
    "href": "reproduction/process_results/process_results.html",
    "title": "Process model results to generate results in article",
    "section": "",
    "text": "This script assumes you have already run the scenarios in models/ and saved those results to .csv files which we here then process to generate the results from the paper."
  },
  {
    "objectID": "reproduction/process_results/process_results.html#set-up",
    "href": "reproduction/process_results/process_results.html#set-up",
    "title": "Process model results to generate results in article",
    "section": "Set-up",
    "text": "Set-up\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n# Path to output folder\noutputs = \"../output\"\n\n# Paths to output files\nfiles &lt;- list(\n  y65_s1_1k =\"output_65yo_scen1_1k.csv\",\n  y65_s1_10k = \"output_65yo_scen1_10k.csv\",\n  y65_s1_100k = \"output_65yo_scen1_100k.csv\",\n  y65_s1_1m = \"output_65yo_scen1_1mil.csv\",\n  y65_s1_2m = \"output_65yo_scen1_2mil.csv\",\n  y65_s2 = \"output_65yo_scen2.csv\",\n  surv_s0 = \"output_surv_scen0.csv\",\n  surv_s0_aorta = \"output_surv_scen0_aaadeath_aortasize.csv\",\n  surv_s1 = \"output_surv_scen1.csv\",\n  surv_s1_aorta = \"output_surv_scen1_aaadeath_aortasize.csv\",\n  surv_s2 = \"output_surv_scen2.csv\",\n  surv_s3 = \"output_surv_scen3.csv\",\n  surv_s4a = \"output_surv_scen4a.csv\",\n  surv_s4b = \"output_surv_scen4b.csv\",\n  surv_s4c = \"output_surv_scen4c.csv\",\n  tab2 = \"tab2.csv\",\n  tab3 = \"tab3.csv\",\n  fig1 = \"fig1.png\",\n  fig2 = \"fig2.png\",\n  fig3 = \"fig3.png\",\n  fig4 = \"fig4.png\",\n  fig5 = \"fig5.png\",\n  intext1 = \"intext1.csv\",\n  suptab2 = \"suptab2.csv\",\n  supfig3 = \"supfig3.png\"\n)\n\n# Apply file.path to each element in list to create path to file in outputs\npaths &lt;- lapply(files, function(filename) file.path(outputs, filename))\n\n\n# Import files\n\ny65_s1_1k &lt;- read.csv(paths$y65_s1_1k)\ny65_s1_10k &lt;- read.csv(paths$y65_s1_10k)\ny65_s1_100k &lt;- read.csv(paths$y65_s1_100k)\ny65_s1_1m &lt;- read.csv(paths$y65_s1_1m)\ny65_s1_2m &lt;- read.csv(paths$y65_s1_2m)\ny65_s2 &lt;- read.csv(paths$y65_s2)\n\nsurv_s0 &lt;- read.csv(paths$surv_s0)\nsurv_s0_aorta &lt;- read.csv(paths$surv_s0_aorta)\nsurv_s1 &lt;- read.csv(paths$surv_s1)\nsurv_s1_aorta &lt;- read.csv(paths$surv_s1_aorta)\nsurv_s2 &lt;- read.csv(paths$surv_s2)\nsurv_s3 &lt;- read.csv(paths$surv_s3)\nsurv_s4a &lt;- read.csv(paths$surv_s4a)\nsurv_s4b &lt;- read.csv(paths$surv_s4b)\nsurv_s4c &lt;- read.csv(paths$surv_s4c)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#functions",
    "href": "reproduction/process_results/process_results.html#functions",
    "title": "Process model results to generate results in article",
    "section": "Functions",
    "text": "Functions\n\nmake_tab &lt;- function(df,\n                     scale_to,\n                     n_person=1000000,\n                     groupvar=\"delayscr\",\n                     decreasing=FALSE) {\n  #' Create section of tables from the article\n  #' \n  #' Create table with a count of excess deaths and excess emergency operations\n  #' with increasing delays in the simulation.\n  #' \n  #' @param df Dataframe - results from the model\n  #' @param scale_to Integer - number of expected people in actual population,\n  #' which we scale our results to (so it reflects the number of outcomes\n  #' anticipated in a population of that size)\n  #' @param n_person Integer - number of people in the simulation model\n  #' @param groupvar String - column with group names (i.e. colour in figure)\n  #' @param decreasing Boolean - whether to sort group decreasing\n  #' \n  #' @return tab2 Dataframe - excess deaths and emergency operations\n  #' \n  #' @examples\n  #' make_tab(y65_s1_1m, 1000000)\n\n  # Sort dataframe by grouping variable (not using dplyr as it didn't work\n  # when needed to parse the string column name with !!, and just did no sort)\n  df_sort &lt;- df[order(df[,groupvar], decreasing=decreasing),]\n  rownames(df_sort) &lt;- NULL\n\n  # Remaining processing steps...\n  tab2 &lt;- df_sort %&gt;%\n    # Calculate the total number of emergency operations\n    mutate(total_emer = emerevar + emeropen) %&gt;%\n    # Keep relevant columns\n    select(!!groupvar, aaadead, total_emer) %&gt;%\n    # Scale to number of deaths if population size was as expected in real life\n    mutate(scaled_dead = round(scale_to*(aaadead/n_person)),\n           scaled_emer = round(scale_to*(total_emer/n_person))) %&gt;%\n    # Calculate excess (compare to time 0, but set negative to 0)\n    mutate(excess_dead = pmax(scaled_dead - first(scaled_dead), 0),\n           excess_emer = pmax(scaled_emer - first(scaled_emer), 0)) %&gt;%\n    # Combine (so its formatted like the article)\n    mutate(excess_dead_emer = paste0(excess_dead, \" (\", excess_emer, \")\")) %&gt;%\n    # Keep relevant columns\n    select(!!groupvar, excess_dead_emer)\n\n  return(tab2)\n}\n\n\nget_pct_change &lt;- function(df, ordervar) {\n  #' Get percentage change in the four outcomes (for use in figures)\n  #' \n  #' @param df Dataframe with results from model\n  #' @param ordervar String - column to order dataframe by (as percentage\n  #' change will be against the first row in the dataframe)\n  #' \n  #' @return fig_df Wide-format dataframe with percentage change added\n  \n  # Sort dataframe by ordering variable (not using dplyr as it didn't work\n  # when needed to parse the string column name with !!, and just did no sort)\n  df_sort &lt;- df[order(df[,ordervar]),]\n  rownames(df_sort) &lt;- NULL\n\n  fig_df &lt;- df_sort %&gt;%\n    # Calculate the total number of emergency and elective operations\n    mutate(total_emer = emerevar + emeropen,\n           total_elec = elecevar + elecopen) %&gt;%\n    # Calculate percentage change from timepoint 0\n    mutate(pct_dead = (aaadead - first(aaadead)) / first(aaadead) * 100,\n           pct_elec = (total_elec - first(total_elec)) / first(total_elec) * 100,\n           pct_emer = (total_emer - first(total_emer)) / first(total_emer) * 100,\n           pct_rupt = (rupt - first(rupt)) / first(rupt) * 100) %&gt;%\n\n  return (fig_df)\n}\n\n\nprepare_fig_df &lt;- function(df, pivotvar) {\n  #' Prepares dataframe for use in making figure by melting and adding labels\n  #' \n  #' @param df Dataframe with results calculated by get_pct_change(), filtered\n  #' to just the relevant columns\n  #' @param pivotvar String - name of column that serves as ID and that we keep\n  #' as a column when melting the dataframe\n  #' \n  #' @return fig_df_long Long-format dataframe ready for creating plots\n\n  # Define labels\n  fig_lab = list(pct_dead = \"AAA deaths\",\n                 pct_elec = \"Elective operations\",\n                 pct_emer = \"Emergency operations\",\n                 pct_rupt = \"Ruptures\")\n\n  # Melt dataframe from wide to long and add labels\n  fig_df_long &lt;- df %&gt;%\n    pivot_longer(-!!pivotvar) %&gt;%\n    mutate(label = recode(name, !!!fig_lab, .default = NA_character_))\n\n  return (fig_df_long)\n}\n\n\nplot_fig &lt;- function(df, xvar, xlab, savepath,\n                     ylab=\"Percentage change in outcome\",\n                     legendtitle=\"Outcome\", linetype=NULL, xbreaks=NULL,\n                     scale_y=list(limits=c(-10, 10), breaks=seq(-10, 10, 2))) {\n  #' Plot the figure using ggplot2\n  #' \n  #' @param df Dataframe created using prepare_fig_df()\n  #' @param xvar String - column with data to plot along x axis\n  #' @param xlab String - label for x axis\n  #' @param savepath String - file path to save image\n  #' @param ylab String - label for y axis\n  #' @legendtitle String - title for figure legend\n  #' @param linetype String - name of coloumn to change style by, or NULL\n  #' @param xbreaks Numeric vector of positions for xbreaks, or NULL (which\n  #' makes it keep the default xbreaks)\n  #' @param scale_y List with inputs to scale_y_continuous. If you do not want\n  #' to specify inputs, then set to NULL\n\n  # Different plot, depending on whether change line type or not\n  if (is.null(linetype)) {\n    p &lt;- ggplot(df, aes(x=!!sym(xvar), y=value, color=label))\n  } else {\n    p &lt;- ggplot(df, aes(x=!!sym(xvar), y=value, color=label,\n                        linetype=!!sym(linetype)))\n  }\n  \n  p &lt;- p +\n    geom_line() +\n    geom_point() +\n    labs(x=xlab, y=ylab, color=legendtitle) +\n    {if(!is.null(scale_y)) do.call(scale_y_continuous, scale_y)} +\n    {if(!is.null(xbreaks))scale_x_continuous(breaks = xbreaks)} +\n    geom_hline(yintercept=0) +\n    theme_bw()\n\n  # Display plot\n  print(p)\n\n  # Save plot\n  ggsave(savepath, width=7, height=5)\n}\n\n\ncombine_with_surv_s0 &lt;- function(df, groupvar){\n  #' Combine your chosen scenario with results from surveillance scenario 0\n  #' \n  #' @param df Dataframe with model results from a given scenario\n  #' @param groupvar Column you will want to group with later, so keep along\n  #' with outcome columns\n  #' @return df_comb Model results from surv scenario 0 + your scenario\n\n  # Define columns to keep (as don't all have same columns, so just filter to\n  # desired - and alternative would've been to filter to common cols)\n  cols = c(groupvar, \"elecevar\", \"elecopen\", \"emerevar\",\n           \"emeropen\", \"aaadead\", \"rupt\")\n\n  # Combine scenario 0 with your chosen scenario\n  df_comb &lt;- rbind(surv_s0 %&gt;% mutate(\"period\" = 0, \"dropoutrate\" = 0) %&gt;% select(any_of(cols)),\n                   df %&gt;% select(any_of(cols)))\n\n  return (df_comb)\n}"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#table-2",
    "href": "reproduction/process_results/process_results.html#table-2",
    "title": "Process model results to generate results in article",
    "section": "Table 2",
    "text": "Table 2\n\ntab2_delay &lt;- make_tab(y65_s1_1m, scale_to=279798) %&gt;%\n  # Keep subset of results\n  filter(delayscr %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(delayscr*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer)\n\ntab2_delay\n\n  months excess_dead_emer\n1     6m            0 (3)\n2    12m            0 (0)\n3    24m            0 (1)\n4    36m          21 (14)\n5    48m          56 (35)\n6    60m         108 (56)\n\n\n\n# Just keep rows in scenario 2 where they had a 6 month delay\ny65_s2_delay &lt;- y65_s2 %&gt;% filter(delayscr==0.5)\n\n# Generate section of table 2\ntab2_attend &lt;- make_tab(y65_s2_delay, scale_to=279798,\n                         groupvar=\"attend\", decreasing=TRUE) %&gt;%\n  # Don't keep base result\n  filter(attend != 0.75) %&gt;%\n  # Convert proportions to percentage\n  mutate(perc = paste0(attend*100, \"%\")) %&gt;%\n  # Keep relevant columns\n  select(perc, excess_dead_emer)\n\ntab2_attend\n\n  perc excess_dead_emer\n1  65%          61 (32)\n2  55%         127 (67)\n3  45%         184 (96)\n\n\nCombine the delay and attend sections to produce table 2\n\n# Add empty rows to attendance and rename the results column to be distinct\ntab2_attend_fill &lt;- rbind(tab2_attend, tab2_attend[0,][rep(NA, 3),]) %&gt;%\n  rename(excess_dead_emer_attend = excess_dead_emer)\n\n# Combine into single dataframe\ntab2 &lt;- cbind(tab2_delay, tab2_attend_fill )%&gt;%\n  # Rename columns to be similar to paper\n  rename(\"Length of delay to invitation\" = months,\n         \"Excess AAA deaths (excess emergency operations) in Model I1*\" = excess_dead_emer,\n         \"Attendance rate at primary scan\" = perc,\n         \"Excess AAA deaths (excess emergency operations) in Model I2*\" = excess_dead_emer_attend)\n\n# Reset index\nrownames(tab2) &lt;- NULL\n\n# Save to csv\nwrite.csv(tab2,paths$tab2, row.names=FALSE)\n\ntab2\n\n  Length of delay to invitation\n1                            6m\n2                           12m\n3                           24m\n4                           36m\n5                           48m\n6                           60m\n  Excess AAA deaths (excess emergency operations) in Model I1*\n1                                                        0 (3)\n2                                                        0 (0)\n3                                                        0 (1)\n4                                                      21 (14)\n5                                                      56 (35)\n6                                                     108 (56)\n  Attendance rate at primary scan\n1                             65%\n2                             55%\n3                             45%\n4                            &lt;NA&gt;\n5                            &lt;NA&gt;\n6                            &lt;NA&gt;\n  Excess AAA deaths (excess emergency operations) in Model I2*\n1                                                      61 (32)\n2                                                     127 (67)\n3                                                     184 (96)\n4                                                         &lt;NA&gt;\n5                                                         &lt;NA&gt;\n6                                                         &lt;NA&gt;"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#varying-the-number-of-people-in-the-simulation-for-scenario-1",
    "href": "reproduction/process_results/process_results.html#varying-the-number-of-people-in-the-simulation-for-scenario-1",
    "title": "Process model results to generate results in article",
    "section": "Varying the number of people in the simulation for scenario 1",
    "text": "Varying the number of people in the simulation for scenario 1\n1000 people\n\nres1k &lt;- make_tab(y65_s1_1k, scale_to=279798, n_person=1000) %&gt;%\n  # Keep subset of results\n  filter(delayscr %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(delayscr*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer)\n\nwrite.csv(res1k, \"../../logbook/posts/2024_07_30/65y_s1_tab2_1k.csv\", row.names=FALSE)\n\nres1k\n\n  months excess_dead_emer\n1     6m           0 (NA)\n2    12m           0 (NA)\n3    24m           0 (NA)\n4    36m           0 (NA)\n5    48m         279 (NA)\n6    60m         279 (NA)\n\n\n10,000 people\n\nres10k &lt;- make_tab(y65_s1_10k, scale_to=279798, n_person=10000) %&gt;%\n  # Keep subset of results\n  filter(delayscr %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(delayscr*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer)\n\nwrite.csv(res10k, \"../../logbook/posts/2024_07_30/65y_s1_tab2_10k.csv\", row.names=FALSE)\n\nres10k\n\n  months excess_dead_emer\n1     6m           56 (0)\n2    12m            0 (0)\n3    24m           56 (0)\n4    36m           56 (0)\n5    48m          140 (0)\n6    60m          196 (0)\n\n\n100,000 people\n\nres100k &lt;- make_tab(y65_s1_100k, scale_to=279798, n_person=100000) %&gt;%\n  # Keep subset of results\n  filter(delayscr %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(delayscr*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer)\n\nwrite.csv(res100k, \"../../logbook/posts/2024_07_30/65y_s1_tab2_100k.csv\", row.names=FALSE)\n\nres100k\n\n  months excess_dead_emer\n1     6m            0 (6)\n2    12m            0 (0)\n3    24m            0 (0)\n4    36m            5 (6)\n5    48m          70 (25)\n6    60m         114 (25)\n\n\n1,000,000 people\n\nres1m &lt;- make_tab(y65_s1_1m, scale_to=279798, n_person=1000000) %&gt;%\n  # Keep subset of results\n  filter(delayscr %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(delayscr*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer)\n\nwrite.csv(res1m, \"../../logbook/posts/2024_07_30/65y_s1_tab2_1m.csv\", row.names=FALSE)\n\nres1m\n\n  months excess_dead_emer\n1     6m            0 (3)\n2    12m            0 (0)\n3    24m            0 (1)\n4    36m          21 (14)\n5    48m          56 (35)\n6    60m         108 (56)\n\n\n2,000,000 people\n\nres2m &lt;- make_tab(y65_s1_2m, scale_to=279798, n_person=2000000) %&gt;%\n  # Keep subset of results\n  filter(delayscr %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(delayscr*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer)\n\nwrite.csv(res2m, \"../../logbook/posts/2024_07_30/65y_s1_tab2_2m.csv\", row.names=FALSE)\n\nres2m\n\n  months excess_dead_emer\n1     6m            0 (0)\n2    12m            0 (0)\n3    24m            0 (0)\n4    36m          20 (11)\n5    48m          50 (31)\n6    60m         105 (60)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#table-3",
    "href": "reproduction/process_results/process_results.html#table-3",
    "title": "Process model results to generate results in article",
    "section": "Table 3",
    "text": "Table 3\nScenario 1\n\n# Combine scenarios 0 and 1\nsurv_s0_s1 &lt;- combine_with_surv_s0(surv_s1, \"period\")\nsurv_s0_s1\n\n  period elecevar elecopen emerevar emeropen aaadead   rupt\n1   0.00   334510   163245    10891    37924  140144 131989\n2   0.25   334510   163245    10891    37924  140144 131989\n3   0.50   334236   163118    10907    37946  140306 132227\n4   1.00   333216   163004    10964    38166  140739 132883\n5   2.00   328549   162312    11201    39157  142942 136199\n6   3.00   319286   160275    11743    41259  147567 143222\n7   4.00   305043   156342    12553    44578  155505 154745\n8   5.00   286144   149707    13812    49536  166776 171525\n\n\n\ntab3_s1 &lt;- make_tab(surv_s0_s1, scale_to = 15376, groupvar = \"period\") %&gt;%\n  # Keep subset of results\n  filter(period %in% c(0.5, 1, 2, 3, 4, 5)) %&gt;%\n  # Convert time from years to months\n  mutate(months = paste0(period*12, \"m\")) %&gt;%\n  # Keep relevant columns\n  select(months, excess_dead_emer) %&gt;%\n  # Rename to match article\n  rename(\"Length of scan suspension\" = months,\n         \"Excess AAA deaths (excess emergency operations) in Model S1\" = excess_dead_emer)\n\ntab3_s1\n\n  Length of scan suspension\n1                        6m\n2                       12m\n3                       24m\n4                       36m\n5                       48m\n6                       60m\n  Excess AAA deaths (excess emergency operations) in Model S1\n1                                                       2 (0)\n2                                                       9 (4)\n3                                                     43 (23)\n4                                                    114 (64)\n5                                                   236 (127)\n6                                                   409 (223)\n\n\nScenario 2\n\n# Combine scenarios 0 and 2.1 or 2.2\nsurv_s0_s2_1y &lt;- combine_with_surv_s0(\n  surv_s2[surv_s2$dropoutperiod == 1,], \"dropoutrate\")\nsurv_s0_s2_2y &lt;- combine_with_surv_s0(\n  surv_s2[surv_s2$dropoutperiod == 2,], \"dropoutrate\")\n\n# Calculate excess deaths and emergency operations\ntab_surv_s2_1y &lt;- make_tab(\n  surv_s0_s2_1y, scale_to = 15376, groupvar = \"dropoutrate\") %&gt;%\n  rename(excess_dead_emer_s21 = excess_dead_emer)\ntab_surv_s2_2y &lt;- make_tab(\n  surv_s0_s2_2y, scale_to = 15376, groupvar = \"dropoutrate\") %&gt;%\n  rename(excess_dead_emer_s22 = excess_dead_emer)\n\n# Combine into single dataframe\ntab3_s2 &lt;- mutate(tab_surv_s2_1y, tab_surv_s2_2y) %&gt;%\n  # Remove result from dropoutrate 0\n  filter(dropoutrate != 0) %&gt;%\n  # Convert dropout rate from proportion to percentage\n  mutate(dropoutrate = paste0(dropoutrate*100, \"%\")) %&gt;%\n  # Rename columns to be similar to paper\n  rename(\"Dropout rate/ annum\" = dropoutrate,\n         \"Excess AAA deaths (excess emergency operations) in Model S2.1\" = excess_dead_emer_s21,\n         \"Excess AAA deaths (excess emergency operations) in Model S2.2\" = excess_dead_emer_s22)\n\ntab3_s2\n\n  Dropout rate/ annum\n1                  8%\n2                 10%\n3                 12%\n4                 15%\n  Excess AAA deaths (excess emergency operations) in Model S2.1\n1                                                       46 (24)\n2                                                       84 (43)\n3                                                      122 (62)\n4                                                      176 (91)\n  Excess AAA deaths (excess emergency operations) in Model S2.2\n1                                                       85 (43)\n2                                                      153 (79)\n3                                                     218 (114)\n4                                                     313 (164)\n\n\nScenario 3\n\n# Get excess deaths and operations\ntab3_s3 &lt;- make_tab(surv_s3, scale_to=15376, groupvar=\"period\") %&gt;%\n  # Reformat period to match article\n  filter(period != 0) %&gt;%\n  mutate(period = paste0(period*12, \"m\")) %&gt;%\n  # Rename results column\n  rename(\"Excess AAA deaths (excess emergency operations) in Model S3\" = excess_dead_emer,\n         \"Length of time at 7cm threshold\" = period)\n  \ntab3_s3\n\n  Length of time at 7cm threshold\n1                              6m\n2                             12m\n3                             24m\n4                             36m\n5                             48m\n6                             60m\n  Excess AAA deaths (excess emergency operations) in Model S3\n1                                                       2 (0)\n2                                                      10 (4)\n3                                                     42 (23)\n4                                                    101 (55)\n5                                                    179 (98)\n6                                                   262 (146)\n\n\nCombine to produce table 3\n\n# Add empty rows to scenario 2 (as has fewer rows in table)\ntab3_s2_na &lt;- rbind(tab3_s2, tab3_s2[0,][rep(NA, 2),])\n\n# Combine into single dataframe\nfull_tab3 &lt;- mutate(tab3_s1, tab3_s3, tab3_s2_na)\n\n# Save to csv\nwrite.csv(full_tab3, paths$tab3, row.names=FALSE)\n\n# Display\nfull_tab3\n\n  Length of scan suspension\n1                        6m\n2                       12m\n3                       24m\n4                       36m\n5                       48m\n6                       60m\n  Excess AAA deaths (excess emergency operations) in Model S1\n1                                                       2 (0)\n2                                                       9 (4)\n3                                                     43 (23)\n4                                                    114 (64)\n5                                                   236 (127)\n6                                                   409 (223)\n  Length of time at 7cm threshold\n1                              6m\n2                             12m\n3                             24m\n4                             36m\n5                             48m\n6                             60m\n  Excess AAA deaths (excess emergency operations) in Model S3\n1                                                       2 (0)\n2                                                      10 (4)\n3                                                     42 (23)\n4                                                    101 (55)\n5                                                    179 (98)\n6                                                   262 (146)\n  Dropout rate/ annum\n1                  8%\n2                 10%\n3                 12%\n4                 15%\n5                &lt;NA&gt;\n6                &lt;NA&gt;\n  Excess AAA deaths (excess emergency operations) in Model S2.1\n1                                                       46 (24)\n2                                                       84 (43)\n3                                                      122 (62)\n4                                                      176 (91)\n5                                                          &lt;NA&gt;\n6                                                          &lt;NA&gt;\n  Excess AAA deaths (excess emergency operations) in Model S2.2\n1                                                       85 (43)\n2                                                      153 (79)\n3                                                     218 (114)\n4                                                     313 (164)\n5                                                          &lt;NA&gt;\n6                                                          &lt;NA&gt;"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#supplementary-table-2",
    "href": "reproduction/process_results/process_results.html#supplementary-table-2",
    "title": "Process model results to generate results in article",
    "section": "Supplementary table 2",
    "text": "Supplementary table 2\n\n# Combine scenarios with scenario 0 (except scenario 1, which we did above)\nsurv_s0_s4a &lt;- combine_with_surv_s0(surv_s4a, \"period\")\nsurv_s0_s4b &lt;- combine_with_surv_s0(surv_s4b, \"period\")\nsurv_s0_s4c &lt;- combine_with_surv_s0(surv_s4c, \"period\")\n\n# Calculate scaled excess deaths and emergencies\nsuptab2_s1 &lt;- make_tab(surv_s0_s1, scale_to=15376, groupvar=\"period\") %&gt;%\n    rename(\"Scan suspension only (S1)\" := excess_dead_emer)\nsuptab2_s21 &lt;- make_tab(surv_s0_s4a, scale_to=15376, groupvar=\"period\") %&gt;%\n    rename(\"+10% dropout/ annum for 1y (S2.1)\" := excess_dead_emer)\nsuptab2_s22 &lt;- make_tab(surv_s0_s4b, scale_to=15376, groupvar=\"period\") %&gt;%\n    rename(\"+10% dropout/ annum for 2y (S2.2)\" := excess_dead_emer)\nsuptab2_s23 &lt;- make_tab(surv_s0_s4c, scale_to=15376, groupvar=\"period\") %&gt;%\n    rename(\"+7cm threshold for 2y (S3)\" := excess_dead_emer)\n\n\n# Combine into single table\nsuptab2 &lt;- mutate(suptab2_s1, suptab2_s21, suptab2_s22, suptab2_s23) %&gt;%\n  # Only keep 6 months +\n  filter(period &gt;= 0.5) %&gt;%\n  # Relabel\n  mutate(period = paste0(period*12, \"m\")) %&gt;%\n  rename(\"Length of scan suspension\" = period)\n\n# Save to csv\nwrite.csv(suptab2, paths$suptab2, row.names=FALSE)\n\n# View dataframe\nsuptab2\n\n  Length of scan suspension Scan suspension only (S1)\n1                        6m                     2 (0)\n2                       12m                     9 (4)\n3                       24m                   43 (23)\n4                       36m                  114 (64)\n5                       48m                 236 (127)\n6                       60m                 409 (223)\n  +10% dropout/ annum for 1y (S2.1) +10% dropout/ annum for 2y (S2.2)\n1                           84 (43)                          153 (79)\n2                           94 (49)                          164 (85)\n3                          126 (67)                         209 (112)\n4                         194 (106)                         274 (147)\n5                         311 (166)                         385 (207)\n6                         478 (259)                         547 (297)\n  +7cm threshold for 2y (S3)\n1                  209 (111)\n2                  209 (111)\n3                  209 (112)\n4                  275 (148)\n5                  386 (207)\n6                  548 (298)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#figure-1",
    "href": "reproduction/process_results/process_results.html#figure-1",
    "title": "Process model results to generate results in article",
    "section": "Figure 1",
    "text": "Figure 1\nProcess results from model into a dataframe to use for plotting.\n\n# Get percentage change\nfig1_pct &lt;- get_pct_change(df=y65_s1_1m, ordervar=\"delayscr\") %&gt;%\n  # Keep relevant columns\n  select(delayscr, starts_with(\"pct_\"))\n\n# Melt and label dataframe\nfig1_df &lt;- prepare_fig_df(fig1_pct, pivotvar=\"delayscr\") %&gt;%\n  # Drop result from 0.25 months (not included in plot)\n  filter(delayscr != 0.25)\n\n# Create and save plot\nplot_fig(fig1_df, xvar=\"delayscr\", xlab=\"Delay in initial invitation (years)\",\n         savepath=paths$fig1)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#figure-2",
    "href": "reproduction/process_results/process_results.html#figure-2",
    "title": "Process model results to generate results in article",
    "section": "Figure 2",
    "text": "Figure 2\nHere, the percentage change in outcome is compared against the outcome at 75% attendance with 0 months delay. Hence, we sort by delay (since that puts that row as first), but when we plot, we then remove that row, and just keep results with 6 months delay.\n\n# Calculate percentage change in four outcomes\nfig2_pct &lt;- get_pct_change(df=y65_s2, ordervar=\"delayscr\") %&gt;%\n  # Drop result whith no delay\n  filter(delayscr!=0) %&gt;%\n  # Keep relevant columns\n  select(attend, starts_with(\"pct_\")) %&gt;%\n  # Convert attendendance from proportion to percentage\n  mutate(attend = attend*100)\n\n# Melt and label dataframe\nfig2_df &lt;- prepare_fig_df(fig2_pct, pivotvar=\"attend\")\n\n# Create and save plot\nplot_fig(fig2_df, xvar=\"attend\", xlab=\"Attendance at initial scan (%)\",\n         xbreaks=seq(45, 75, 5), savepath=paths$fig2)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#figure-3",
    "href": "reproduction/process_results/process_results.html#figure-3",
    "title": "Process model results to generate results in article",
    "section": "Figure 3",
    "text": "Figure 3\n\nfig3_pct &lt;- get_pct_change(df=surv_s0_s1, ordervar=\"period\") %&gt;%\n  # Drop years 4 and 5\n  filter(period &lt;= 3) %&gt;%\n  # Keep relevant columns\n  select(period, starts_with(\"pct_\"))\n\n# Melt and label dataframe\nfig3_df &lt;- prepare_fig_df(fig3_pct, pivotvar=\"period\")\n\nplot_fig(fig3_df, xvar=\"period\", xlab=\"Surveillance scan suspension (years)\",\n         savepath=paths$fig3)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#figure-4",
    "href": "reproduction/process_results/process_results.html#figure-4",
    "title": "Process model results to generate results in article",
    "section": "Figure 4",
    "text": "Figure 4\n\n# Get percentage change for each (when dropout is over 1 year, or over 2 years)\nfig4_1_pct &lt;- get_pct_change(surv_s0_s2_1y, \"dropoutrate\") %&gt;%\n  select(dropoutrate, starts_with(\"pct_\")) %&gt;%\n  mutate(period = \"1 year\")\n\nfig4_2_pct &lt;- get_pct_change(surv_s0_s2_2y, \"dropoutrate\") %&gt;%\n  select(dropoutrate, starts_with(\"pct_\")) %&gt;%\n  mutate(period = \"2 years\")\n\n# Combine into a single dataframe\nfig4_pct &lt;- rbind(fig4_1_pct, fig4_2_pct) %&gt;%\n  # Remove dropoutrate 0 as not included in plot\n  filter(dropoutrate != 0)\n\n# Melt and label dataframe\nfig4_df &lt;- prepare_fig_df(\n  fig4_pct, pivotvar=c(\"dropoutrate\", \"period\")) %&gt;%\n  # Convert dropout from proportion to percentage\n  mutate(dropoutrate = dropoutrate*100)\n\nfig4_df\n\n# A tibble: 32 √ó 5\n   dropoutrate period name     value label               \n         &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;               \n 1           8 1 year pct_dead  2.14 AAA deaths          \n 2           8 1 year pct_elec -1.33 Elective operations \n 3           8 1 year pct_emer  3.19 Emergency operations\n 4           8 1 year pct_rupt  3.28 Ruptures            \n 5          10 1 year pct_dead  3.92 AAA deaths          \n 6          10 1 year pct_elec -2.38 Elective operations \n 7          10 1 year pct_emer  5.81 Emergency operations\n 8          10 1 year pct_rupt  5.98 Ruptures            \n 9          12 1 year pct_dead  5.65 AAA deaths          \n10          12 1 year pct_elec -3.42 Elective operations \n# ‚Ñπ 22 more rows\n\n\n\nplot_fig(fig4_df, xvar=\"dropoutrate\", xlab=\"Dropout/annum(%)\",\n         scale_y=NULL, savepath=paths$fig4, linetype=\"period\")"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#figure-5",
    "href": "reproduction/process_results/process_results.html#figure-5",
    "title": "Process model results to generate results in article",
    "section": "Figure 5",
    "text": "Figure 5\n\n# Get percentage change in outcomes from period 0 thresh 5.5\nfig5_pct &lt;- get_pct_change(surv_s3, \"period\") %&gt;%\n  select(period, starts_with(\"pct_\"))\n\n# Melt and label dataframe\nfig4_df &lt;- prepare_fig_df(fig5_pct, pivotvar=c(\"period\")) %&gt;%\n  # Filter to max 3 years (as article does not include above that in figure)\n  filter(period &lt;= 3)\n\n# Create figure\nplot_fig(fig4_df, xvar=\"period\", xlab=\"Time at 7cm threshold (years)\",\n         savepath=paths$fig5)"
  },
  {
    "objectID": "reproduction/process_results/process_results.html#supplementary-figure-3",
    "href": "reproduction/process_results/process_results.html#supplementary-figure-3",
    "title": "Process model results to generate results in article",
    "section": "Supplementary figure 3",
    "text": "Supplementary figure 3\n\n# Get percentage change for each scenario (have to calculate seperately)\nsupfig3_s1_pct &lt;- get_pct_change(surv_s0_s1, \"period\") %&gt;%\n  mutate(scenario = \"S1\")\nsupfig3_s21_pct &lt;- get_pct_change(surv_s0_s4a, \"period\") %&gt;%\n  mutate(scenario = \"S1+S2.1\")\nsupfig3_s22_pct &lt;- get_pct_change(surv_s0_s4b, \"period\") %&gt;%\n  mutate(scenario = \"S1+S2.2\")\nsupfig3_s3_pct &lt;- get_pct_change(surv_s0_s4c, \"period\") %&gt;%\n  mutate(scenario = \"S1+S2.2+S3\")\n\n# Combine into a single dataframe\nsupfig3_pct &lt;- rbind(\n  supfig3_s1_pct, supfig3_s21_pct, supfig3_s22_pct, supfig3_s3_pct) %&gt;%\n  # Filter to relevant columns\n  select(period, scenario, pct_dead)\n\n# Melt and label dataframe\nsupfig3_df &lt;- prepare_fig_df(supfig3_pct, pivotvar=c(\"period\", \"scenario\")) %&gt;%\n  # Modify so scenarios are the label\n  select(-c(\"name\", \"label\")) %&gt;%\n  rename(\"label\" = scenario) %&gt;%\n  # Remove result from timepoint 0\n  filter(period != 0)\n\n# Create figure\nplot_fig(supfig3_df, xvar=\"period\",\n         xlab=\"Surveillance scan suspension (years)\",\n         ylab=\"Percentage change in AAA deaths\",\n         savepath=paths$supfig3, legendtitle=\"Scenario\",\n         scale_y=list(limits=c(0, 30), breaks=seq(0, 30, 5)))\n\n\n\n\n\n\n\n\n\n## In-text result 1\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine the scenario 0 and 1 results\naorta_size &lt;- rbind(surv_s0_aorta, surv_s1_aorta)\n\n# Empty list to store results\naorta_res &lt;- data.frame()\n\nfor (size in c(\"small\", \"med\", \"large\")) {\n  # Filter to aorta size\n  aorta_df &lt;- aorta_size %&gt;%\n    filter(aorta_size == size)\n\n  aorta_res &lt;- rbind(aorta_res, aorta_df %&gt;%\n    # Scale results to population of 15,376\n    mutate(dead_scaled = round(15376*(aaadead/n))) %&gt;%\n    # Calculate excess deaths (compared with period = 0) and percentage change\n    arrange(period) %&gt;%\n    mutate(extra_deaths = dead_scaled - first(dead_scaled),\n           pct_change = round(\n             (dead_scaled - first(dead_scaled)) / first(dead_scaled) * 100, 2)))\n}\n\n# Tidy and reformat the results table\nintext1 &lt;- aorta_res %&gt;%\n  select(period, aorta_size, dead_scaled, extra_deaths, pct_change) %&gt;%\n  arrange(period) %&gt;%\n  rename(\"years_of_surveillance_suspension\" = period)\n\nwrite.csv(intext1, paths$intext1, row.names=FALSE)\n\nintext1\n\n  years_of_surveillance_suspension aorta_size dead_scaled extra_deaths\n1                                0      small        1750            0\n2                                0        med         225            0\n3                                0      large         180            0\n4                                1      small        1750            0\n5                                1        med         227            2\n6                                1      large         187            7\n7                                2      small        1755            5\n8                                2        med         235           10\n9                                2      large         207           27\n  pct_change\n1       0.00\n2       0.00\n3       0.00\n4       0.00\n5       0.89\n6       3.89\n7       0.29\n8       4.44\n9      15.00\n\n:::"
  },
  {
    "objectID": "quarto_site/license.html",
    "href": "quarto_site/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "This repository is licensed under the GNU Lesser General Public License v3.0.\n\n\n\n\n\n\nView license\n\n\n\n\n\n               GNU LESSER GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\nCopyright (C) 2007 Free Software Foundation, Inc.¬†https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nThis version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below.\n\nAdditional Definitions.\n\nAs used herein, ‚Äúthis License‚Äù refers to version 3 of the GNU Lesser General Public License, and the ‚ÄúGNU GPL‚Äù refers to version 3 of the GNU General Public License.\n‚ÄúThe Library‚Äù refers to a covered work governed by this License, other than an Application or a Combined Work as defined below.\nAn ‚ÄúApplication‚Äù is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library.\nA ‚ÄúCombined Work‚Äù is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the ‚ÄúLinked Version‚Äù.\nThe ‚ÄúMinimal Corresponding Source‚Äù for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version.\nThe ‚ÄúCorresponding Application Code‚Äù for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work.\n\nException to Section 3 of the GNU GPL.\n\nYou may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL.\n\nConveying Modified Versions.\n\nIf you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version:\n\nunder this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or\nunder the GNU GPL, with none of the additional permissions of this License applicable to that copy.\n\n\nObject Code Incorporating Material from Library Header Files.\n\nThe object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following:\n\nGive prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License.\nAccompany the object code with a copy of the GNU GPL and this license document.\n\n\nCombined Works.\n\nYou may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following:\n\nGive prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License.\nAccompany the Combined Work with a copy of the GNU GPL and this license document.\nFor a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document.\nDo one of the following:\n\nConvey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.\nUse a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user‚Äôs computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version.\n\nProvide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.)\n\n\nCombined Libraries.\n\nYou may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following:\n\nAccompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License.\nGive prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.\n\n\nRevised Versions of the GNU Lesser General Public License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License ‚Äúor any later version‚Äù applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation.\nIf the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy‚Äôs public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.\n\n\n\nThis is aligned with the original study, who shared their code under the GNU Lesser General Public License v3.0.\n\n\n\n\n\n\nView license\n\n\n\n\n\n               GNU LESSER GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\nCopyright (C) 2007 Free Software Foundation, Inc.¬†https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nThis version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below.\n\nAdditional Definitions.\n\nAs used herein, ‚Äúthis License‚Äù refers to version 3 of the GNU Lesser General Public License, and the ‚ÄúGNU GPL‚Äù refers to version 3 of the GNU General Public License.\n‚ÄúThe Library‚Äù refers to a covered work governed by this License, other than an Application or a Combined Work as defined below.\nAn ‚ÄúApplication‚Äù is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library.\nA ‚ÄúCombined Work‚Äù is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the ‚ÄúLinked Version‚Äù.\nThe ‚ÄúMinimal Corresponding Source‚Äù for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version.\nThe ‚ÄúCorresponding Application Code‚Äù for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work.\n\nException to Section 3 of the GNU GPL.\n\nYou may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL.\n\nConveying Modified Versions.\n\nIf you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version:\n\nunder this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or\nunder the GNU GPL, with none of the additional permissions of this License applicable to that copy.\n\n\nObject Code Incorporating Material from Library Header Files.\n\nThe object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following:\n\nGive prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License.\nAccompany the object code with a copy of the GNU GPL and this license document.\n\n\nCombined Works.\n\nYou may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following:\n\nGive prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License.\nAccompany the Combined Work with a copy of the GNU GPL and this license document.\nFor a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document.\nDo one of the following:\n\nConvey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.\nUse a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user‚Äôs computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version.\n\nProvide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.)\n\n\nCombined Libraries.\n\nYou may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following:\n\nAccompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License.\nGive prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.\n\n\nRevised Versions of the GNU Lesser General Public License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License ‚Äúor any later version‚Äù applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation.\nIf the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy‚Äôs public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.\n\n\n\nThe original study was published in the journal ‚ÄúPLoS ONE‚Äù. They distributed the article under the Creative Commons Attribution License.\n\n\n\n\n\n\nView copyright statement from journal\n\n\n\n\n\n‚ÄúCopyright: ¬© 2021 Kim et al.¬†This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.‚Äù\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253327"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducing Kim et al. 2021",
    "section": "",
    "text": "This book captures the reproduction of:\n\nKim LG, Sweeting MJ, Armer M, Jacomelli J, Nasim A, Harrison SC. Modelling the impact of changes to abdominal aortic aneurysm screening and treatment services in England during the COVID-19 pandemic. PLoS ONE 16(6): e0253327 (2021). https://doi.org/10.1371/journal.pone.0253327.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Reproducing Kim et al. 2021",
    "section": "",
    "text": "This book captures the reproduction of:\n\nKim LG, Sweeting MJ, Armer M, Jacomelli J, Nasim A, Harrison SC. Modelling the impact of changes to abdominal aortic aneurysm screening and treatment services in England during the COVID-19 pandemic. PLoS ONE 16(6): e0253327 (2021). https://doi.org/10.1371/journal.pone.0253327.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#project-team",
    "href": "index.html#project-team",
    "title": "Reproducing Kim et al. 2021",
    "section": "Project team",
    "text": "Project team\n\n\nConducting this reproduction:\n\nAmy Heather \n\nProviding support during the reproduction:\n\nThomas Monks \nAlison Harper \n\nOther members of the team on STARS:\n\nNavonil Mustafee \nAndrew Mayne"
  },
  {
    "objectID": "index.html#protocol",
    "href": "index.html#protocol",
    "title": "Reproducing Kim et al. 2021",
    "section": "Protocol",
    "text": "Protocol\nThe protocol for this work is summarised in the diagram below and archived on Zenodo:\n\nHeather, A., Monks, T., Harper, A., Mustafee, N., & Mayne, A. (2024). Protocol for assessing the computational reproducibility of discrete-event simulation models on STARS. Zenodo. https://doi.org/10.5281/zenodo.12179846.\n\n\n\n\nWorkflow for computational reproducibility assessment"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Reproducing Kim et al. 2021",
    "section": "Citation",
    "text": "Citation\nAPA: Heather A., Monks T., Harper A. (2024). STARS: Computational reproducibility of Kim et al.¬†2021 (version 0.1.0). URL: https://github.com/pythonhealthdatascience/stars-reproduce-kim-2021\nSee CITATION.cff and citation_bibtex.bib for alternative formats."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reproducing Kim et al. 2021",
    "section": "License",
    "text": "License\nSee License page."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning. Dates formatted as YYYY-MM-DD as per ISO standard.\n\n\nSet up repository and defined scope of reproduction.\n\n\n\nCode from original study\nArticle and supplementary materials\nPlanned scope for reproduction\n\n\n\n\n\nModified template to be about Kim et al.¬†2021"
  },
  {
    "objectID": "CHANGELOG.html#v0.1.0---2024-07-29",
    "href": "CHANGELOG.html#v0.1.0---2024-07-29",
    "title": "Changelog",
    "section": "",
    "text": "Set up repository and defined scope of reproduction.\n\n\n\nCode from original study\nArticle and supplementary materials\nPlanned scope for reproduction\n\n\n\n\n\nModified template to be about Kim et al.¬†2021"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html",
    "href": "quarto_site/reproduction_readme.html",
    "title": "README for reproduction",
    "section": "",
    "text": "This study adapts a previously developed discrete-event simulation model for abdominal aortic aneurysm (AAA) screening of men in England. It aims to explore different approaches to resuming screening and surgical repair for AAA, as these survives were paused or substantially reduced during COVID-19 due to concerns about virus transmission.\n\n\n\nIn this assessment, we attempted to reproduce 10 items: 6 figures, 3 tables and 1 in-text result.\n\n\n\n\n\n‚îú‚îÄ‚îÄ docker\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ functions\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ input\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ models\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ output\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ process_results\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ renv\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ tests\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ .Rprofile\n‚îú‚îÄ‚îÄ DESCRIPTION\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ kim2021.Rproj\n‚îî‚îÄ‚îÄ renv.lock\n\ndocker/ - Instructions for creation of docker container.\nfunctions/ - Discrete-event simulation model code\ninput/ - Input parameters for the model\nmodels/ - Scripts to run each scenario\noutput/ - Output files from the scripts (e.g.¬†.csv, .png)\nprocess_results/\nrenv/ - Instructions for creation of R environment\ntests/ - Test to check that the model produces consistent results with our reproduction\n.Rprofile - Activates R environment\nDESCRIPTION - Lists packages that we installed into environment (their dependencies will have also been installed)\nREADME.md - This file!\nkim2021.Rproj - Project settings, which specify the Python virtual environment to use when building pages from the Quarto site that include Python. If you choose to build the Quarto site (and not just run the reproduction files in this folder), you will want to update this to a path on your machine (which you can do easily by opening this file in RStudio)\nrenv.lock - Lists R version and all packages in the R environment\n\n\n\n\nBefore you can run the model, you will need to create an R environment with the correct version of R and the specified packages.\n\n\nAn renv environment has been provided. To create this environment locally on your machine, you should open the R project with the R environment loaded, and then run:\nrenv::restore()\nIn renv.lock, you will see the version of R listed. However, renv will not install this for you, so you will need to switch to this yourself if you wish to also use the same version of R. This reproduction has been run in R 4.4.1. If you use a different version of R, there‚Äôs a chance that it might be incompatible, or that you may encounter difficulties installing the specified package versions in that version of R.\n\n\n\nA Dockerfile is provided, which you can use to build the Docker image. The docker image will include the correct version of R, the required packages and their versions, and an installation of RStudio which you can run from your browser. It will also include the scripts and outputs from this directory. For this option and option C, you‚Äôll need to ensure that docker is installed on your machine.\nTo create the docker image and then open up RStudio:\n\nIn the terminal, navigate to the parent directory of your reproduction/ folder\nBuild the image:\n\nsudo docker build --tag kim2021 . -f ./reproduction/docker/Dockerfile\n\nCreate container and open RStudio in your browser:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name kim2021_docker kim2021\n\n\n\nA pre-built image is available on the GitHub container registry. To use it:\n\nCreate a Personal Access Token (Classic) for your GitHub account with write:packages and delete:packages access\nOn terminal, run the following command and then enter your sudo password (if prompted), followed by the token just generated (which acts as your GitHub password)\n\nsudo docker login ghcr.io -u githubusername\n\nDownload the image:\n\nsudo docker pull ghcr.io/pythonhealthdatascience/kim2021\n\nCreate container and open RStudio:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name kim2021_docker ghcr.io/pythonhealthdatascience/kim2021:latest\n\n\n\n\n\n\nTo run all the model scenarios, open and execute the provided .R files in models/NAASP_COVID_modelling/. You should run these using source().\nTo process the model outputs and produce the tables and figures from the paper, run the file process_results/process_results.Rmd.\n\n\n\n\nA small version of one the model scenarios is provided as a test within tests/testthat. You can run this scenario by running the following command from your R console whilst in the reproduction/ directory:\ntestthat::test_dir(\"tests/testthat\")\nThe test should only take about 5-10 seconds depending on your machine specs.\n\n\n\n\nWithin this reproduction, due to long run times, the model was run on a remote machine. This was an Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux. We also reduced the number of patients in the simulation from 10 million to 1 million, to improve run times. In total, it took 6 hours 53 minutes to run all the model scenarios. This included:\n\n65 year old scenario 0 - 3 minutes 32 seconds (212 seconds)\n65 year old scenario 1 - 29 minutes 14 seconds (1754 seconds)\n65 year old scenario 2 - 19 minutes 7 seconds (1147 seconds)\nSurveillance scenario 0 - 4 minutes 28 seconds (268 seconds)\nSurveillance scenario 1 - 1 hour 10 minutes 11 seconds (4211 seconds)\nSurveillance scenario 2 - 1 hour 39 minutes 39 seconds (5979 seconds)\nSurveillance scenario 3 - 1 hour 21 minutes 40 seconds (4900 seconds)\nSurveillance scenario 4a - 35 minutes 23 seconds (2123 seconds)\nSurveillance scenario 4b - 34 minutes 57 seconds (2097 seconds)\nSurveillance scenario 4c - 35 minutes 14 seconds (2114 seconds)\n\nYou can expect the runtime to be notably longer on machines with lower specs than this. For example, I ran surveillance scenario 0 on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux. The runtime increased from 4 minutes 28 seconds up to 21 minutes 59 seconds.\n\n\n\n\nTo cite the original study, please refer to the reference above. To cite this reproduction, please refer to the CITATION.cff file in the parent folder.\n\n\n\nThis repository is licensed under the GNU Lesser General Public License v3.0."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#model-summary",
    "href": "quarto_site/reproduction_readme.html#model-summary",
    "title": "README for reproduction",
    "section": "",
    "text": "This study adapts a previously developed discrete-event simulation model for abdominal aortic aneurysm (AAA) screening of men in England. It aims to explore different approaches to resuming screening and surgical repair for AAA, as these survives were paused or substantially reduced during COVID-19 due to concerns about virus transmission."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "href": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "title": "README for reproduction",
    "section": "",
    "text": "In this assessment, we attempted to reproduce 10 items: 6 figures, 3 tables and 1 in-text result."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "href": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "title": "README for reproduction",
    "section": "",
    "text": "‚îú‚îÄ‚îÄ docker\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ functions\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ input\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ models\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ output\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ process_results\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ renv\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ tests\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ .Rprofile\n‚îú‚îÄ‚îÄ DESCRIPTION\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ kim2021.Rproj\n‚îî‚îÄ‚îÄ renv.lock\n\ndocker/ - Instructions for creation of docker container.\nfunctions/ - Discrete-event simulation model code\ninput/ - Input parameters for the model\nmodels/ - Scripts to run each scenario\noutput/ - Output files from the scripts (e.g.¬†.csv, .png)\nprocess_results/\nrenv/ - Instructions for creation of R environment\ntests/ - Test to check that the model produces consistent results with our reproduction\n.Rprofile - Activates R environment\nDESCRIPTION - Lists packages that we installed into environment (their dependencies will have also been installed)\nREADME.md - This file!\nkim2021.Rproj - Project settings, which specify the Python virtual environment to use when building pages from the Quarto site that include Python. If you choose to build the Quarto site (and not just run the reproduction files in this folder), you will want to update this to a path on your machine (which you can do easily by opening this file in RStudio)\nrenv.lock - Lists R version and all packages in the R environment\n\n\n\n\nBefore you can run the model, you will need to create an R environment with the correct version of R and the specified packages.\n\n\nAn renv environment has been provided. To create this environment locally on your machine, you should open the R project with the R environment loaded, and then run:\nrenv::restore()\nIn renv.lock, you will see the version of R listed. However, renv will not install this for you, so you will need to switch to this yourself if you wish to also use the same version of R. This reproduction has been run in R 4.4.1. If you use a different version of R, there‚Äôs a chance that it might be incompatible, or that you may encounter difficulties installing the specified package versions in that version of R.\n\n\n\nA Dockerfile is provided, which you can use to build the Docker image. The docker image will include the correct version of R, the required packages and their versions, and an installation of RStudio which you can run from your browser. It will also include the scripts and outputs from this directory. For this option and option C, you‚Äôll need to ensure that docker is installed on your machine.\nTo create the docker image and then open up RStudio:\n\nIn the terminal, navigate to the parent directory of your reproduction/ folder\nBuild the image:\n\nsudo docker build --tag kim2021 . -f ./reproduction/docker/Dockerfile\n\nCreate container and open RStudio in your browser:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name kim2021_docker kim2021\n\n\n\nA pre-built image is available on the GitHub container registry. To use it:\n\nCreate a Personal Access Token (Classic) for your GitHub account with write:packages and delete:packages access\nOn terminal, run the following command and then enter your sudo password (if prompted), followed by the token just generated (which acts as your GitHub password)\n\nsudo docker login ghcr.io -u githubusername\n\nDownload the image:\n\nsudo docker pull ghcr.io/pythonhealthdatascience/kim2021\n\nCreate container and open RStudio:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name kim2021_docker ghcr.io/pythonhealthdatascience/kim2021:latest\n\n\n\n\n\n\nTo run all the model scenarios, open and execute the provided .R files in models/NAASP_COVID_modelling/. You should run these using source().\nTo process the model outputs and produce the tables and figures from the paper, run the file process_results/process_results.Rmd.\n\n\n\n\nA small version of one the model scenarios is provided as a test within tests/testthat. You can run this scenario by running the following command from your R console whilst in the reproduction/ directory:\ntestthat::test_dir(\"tests/testthat\")\nThe test should only take about 5-10 seconds depending on your machine specs."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "href": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "title": "README for reproduction",
    "section": "",
    "text": "Within this reproduction, due to long run times, the model was run on a remote machine. This was an Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux. We also reduced the number of patients in the simulation from 10 million to 1 million, to improve run times. In total, it took 6 hours 53 minutes to run all the model scenarios. This included:\n\n65 year old scenario 0 - 3 minutes 32 seconds (212 seconds)\n65 year old scenario 1 - 29 minutes 14 seconds (1754 seconds)\n65 year old scenario 2 - 19 minutes 7 seconds (1147 seconds)\nSurveillance scenario 0 - 4 minutes 28 seconds (268 seconds)\nSurveillance scenario 1 - 1 hour 10 minutes 11 seconds (4211 seconds)\nSurveillance scenario 2 - 1 hour 39 minutes 39 seconds (5979 seconds)\nSurveillance scenario 3 - 1 hour 21 minutes 40 seconds (4900 seconds)\nSurveillance scenario 4a - 35 minutes 23 seconds (2123 seconds)\nSurveillance scenario 4b - 34 minutes 57 seconds (2097 seconds)\nSurveillance scenario 4c - 35 minutes 14 seconds (2114 seconds)\n\nYou can expect the runtime to be notably longer on machines with lower specs than this. For example, I ran surveillance scenario 0 on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux. The runtime increased from 4 minutes 28 seconds up to 21 minutes 59 seconds."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#citation",
    "href": "quarto_site/reproduction_readme.html#citation",
    "title": "README for reproduction",
    "section": "",
    "text": "To cite the original study, please refer to the reference above. To cite this reproduction, please refer to the CITATION.cff file in the parent folder."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#license",
    "href": "quarto_site/reproduction_readme.html#license",
    "title": "README for reproduction",
    "section": "",
    "text": "This repository is licensed under the GNU Lesser General Public License v3.0."
  },
  {
    "objectID": "quarto_site/study_publication.html",
    "href": "quarto_site/study_publication.html",
    "title": "Publication",
    "section": "",
    "text": "Kim et al. (2021)"
  },
  {
    "objectID": "quarto_site/study_publication.html#code-and-data",
    "href": "quarto_site/study_publication.html#code-and-data",
    "title": "Publication",
    "section": "Code and data",
    "text": "Code and data\nCopied into this repository: https://github.com/pythonhealthdatascience/stars-reproduce-kim-2021/tree/main/original_study/AAA_DES_model\nOriginal repository link: https://github.com/mikesweeting/AAA_DES_model"
  },
  {
    "objectID": "quarto_site/study_publication.html#journal-article",
    "href": "quarto_site/study_publication.html#journal-article",
    "title": "Publication",
    "section": "Journal article",
    "text": "Journal article\nThe article is published with PLoS ONE and has a permissive copyright statement:\n\n‚ÄúCopyright: ¬© 2021 Kim et al.¬†This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.‚Äù\n\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253327"
  },
  {
    "objectID": "quarto_site/study_publication.html#supplementary-materials",
    "href": "quarto_site/study_publication.html#supplementary-materials",
    "title": "Publication",
    "section": "Supplementary materials",
    "text": "Supplementary materials"
  },
  {
    "objectID": "evaluation/artefacts.html",
    "href": "evaluation/artefacts.html",
    "title": "STARS framework",
    "section": "",
    "text": "This page evaluates the extent to which the original study meets the recommendations from the STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\nOf the 8 essential STARS components:\n\n4 were met fully (‚úÖ)\n1 was met partially (üü°)\n3 was not met (‚ùå)\n\nOf the 5 optional STARS components:\n\n5 were not met (‚ùå)\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\nMet by study?\nEvidence/location\n\n\n\n\nEssential components\n\n\n\n\n\nOpen license\nFree and open-source software (FOSS) license (e.g.¬†MIT, GNU Public License (GPL))\n‚úÖ Fully\nLGPL-3.0\n\n\nDependency management\nSpecify software libraries, version numbers and sources (e.g.¬†dependency management tools like virtualenv, conda, poetry)\nüü° Partially\nIncludes installation instructions/commandas at start of scripts, listing the required packages, but not versions\n\n\nFOSS model\nCoded in FOSS language (e.g.¬†R, Julia, Python)\n‚úÖ Fully\nR\n\n\nMinimum documentation\nMinimal instructions (e.g.¬†in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n‚úÖ Fully\nIncludes a README which describe the repository structure (e.g.¬†where input parameters are located, where script is located), which directs to an example file that explains how to run the model and vary inputs\n\n\nORCID\nORCID for each study author\n‚ùå Not met\n-\n\n\nCitation information\nInstructions on how to cite the research artefact (e.g.¬†CITATION.cff file)\n‚ùå Not met\n-\n\n\nRemote code repository\nCode available in a remote code repository (e.g.¬†GitHub, GitLab, BitBucket)\n‚úÖ Fully\nhttps://github.com/mikesweeting/AAA_DES_model\n\n\nOpen science archive\nCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g.¬†Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n‚ùå Not met\n-\n\n\nOptional components\n\n\n\n\n\nEnhanced documentation\nOpen and high quality documentation on how the model is implemented and works (e.g.¬†via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:‚Ä¢ Plain english summary of project and model‚Ä¢ Clarifying license‚Ä¢ Citation instructions‚Ä¢ Contribution instructions‚Ä¢ Model installation instructions‚Ä¢ Structured code walk through of model‚Ä¢ Documentation of modelling cycle using TRACE‚Ä¢ Annotated simulation reporting guidelines‚Ä¢ Clear description of model validation including its intended purpose\n‚ùå Not met\n-\n\n\nDocumentation hosting\nHost documentation (e.g.¬†with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n‚ùå Not met\n-\n\n\nOnline coding environment\nProvide an online environment where users can run and change code (e.g.¬†BinderHub, Google Colaboratory, Deepnote)\n‚ùå Not met\n-\n\n\nModel interface\nProvide web application interface to the model so it is accessible to less technical simulation users\n‚ùå Not met\n-\n\n\nWeb app hosting\nHost web app online (e.g.¬†Streamlit Community Cloud, ShinyApps hosting)\n‚ùå Not met\n-\n\n\n\n\n\n\n\nReferences\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. ‚ÄúTowards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.‚Äù Journal of Simulation 0 (0): 1‚Äì20. https://doi.org/10.1080/17477778.2024.2347882."
  },
  {
    "objectID": "evaluation/reproduction_success.html",
    "href": "evaluation/reproduction_success.html",
    "title": "Reproduction success",
    "section": "",
    "text": "Of the 10 items in the scope, 100% (10 out of 10) were considered to be successfully reproduced.\nAs cited throughout, images on this page are sourced from Kim et al. (2021)\nNote: This items have been reproduced with simulations containing 1,000,000 people (instead of 10,000,000) as that is very computationally expensive. We felt that, with 1,000,000 people, results are reasonably similar enough to be satisified with reproduction, whilst still being feasible to run on our machines."
  },
  {
    "objectID": "evaluation/reproduction_success.html#time-to-completion",
    "href": "evaluation/reproduction_success.html#time-to-completion",
    "title": "Reproduction success",
    "section": "Time-to-completion",
    "text": "Time-to-completion\nNon-interactive plot:\n\n\n\n\n\n\n\n\n\nInteractive plot:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-1",
    "href": "evaluation/reproduction_success.html#figure-1",
    "title": "Reproduction success",
    "section": "Figure 1",
    "text": "Figure 1\n\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#table-2",
    "href": "evaluation/reproduction_success.html#table-2",
    "title": "Reproduction success",
    "section": "Table 2",
    "text": "Table 2\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:\n\n\n\n\n\n\n\n\n\n\nLength of delay to invitation\nExcess AAA deaths (excess emergency operations) in Model I1*\nAttendance rate at primary scan\nExcess AAA deaths (excess emergency operations) in Model I2*\n\n\n\n\n0\n6m\n0 (3)\n65%\n61 (32)\n\n\n1\n12m\n0 (0)\n55%\n127 (67)\n\n\n2\n24m\n0 (1)\n45%\n184 (96)\n\n\n3\n36m\n21 (14)\nNaN\nNaN\n\n\n4\n48m\n56 (35)\nNaN\nNaN\n\n\n5\n60m\n108 (56)\nNaN\nNaN"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-2",
    "href": "evaluation/reproduction_success.html#figure-2",
    "title": "Reproduction success",
    "section": "Figure 2",
    "text": "Figure 2\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-3",
    "href": "evaluation/reproduction_success.html#figure-3",
    "title": "Reproduction success",
    "section": "Figure 3",
    "text": "Figure 3\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#in-text-result-1",
    "href": "evaluation/reproduction_success.html#in-text-result-1",
    "title": "Reproduction success",
    "section": "In-text result 1",
    "text": "In-text result 1\nIn bold are the items not captured in Table 3 or Figure 3:\n‚ÄúSuspending ultrasound scans in the surveillance cohort could result in 9 (0.4% increase) additional AAA-related deaths if scans were suspended for one year (Table 3, Fig 3). Of these, 2 (1% increase) are in the sub-group measuring 4.5‚Äì4.9 cm at the start of the pandemic and 7 (8% increase) in the sub-group measuring 5.0‚Äì5.4 cm; &lt;0.1 are in the 3.0‚Äì4.4 cm sub-group. More pronounced effects are evident for suspension for two years and beyond. Suspending surveillance for two years could result in 40 excess AAA-related deaths overall; a 1.9% increase over the lifetime of the surveillance cohort. Of these, 1 is in the 3.0‚Äì4.4 cm sub-group and 17 (7% increase) in the 4.5‚Äì4.9cm sub-group. However, the remaining 22 excess deaths are in the 5.0‚Äì5.4cm range, corresponding to a 24% increase in AAA-related deaths in this sub-group.‚Äù\nReproduction:\n\n\n\n\n\n\n\n\n\n\nyears_of_surveillance_suspension\naorta_size\ndead_scaled\nextra_deaths\npct_change\n\n\n\n\n0\n0\nsmall\n1750\n0\n0.00\n\n\n1\n0\nmed\n225\n0\n0.00\n\n\n2\n0\nlarge\n180\n0\n0.00\n\n\n3\n1\nsmall\n1750\n0\n0.00\n\n\n4\n1\nmed\n227\n2\n0.89\n\n\n5\n1\nlarge\n187\n7\n3.89\n\n\n6\n2\nsmall\n1755\n5\n0.29\n\n\n7\n2\nmed\n235\n10\n4.44\n\n\n8\n2\nlarge\n207\n27\n15.00"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-4",
    "href": "evaluation/reproduction_success.html#figure-4",
    "title": "Reproduction success",
    "section": "Figure 4",
    "text": "Figure 4\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-5",
    "href": "evaluation/reproduction_success.html#figure-5",
    "title": "Reproduction success",
    "section": "Figure 5",
    "text": "Figure 5\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#table-3",
    "href": "evaluation/reproduction_success.html#table-3",
    "title": "Reproduction success",
    "section": "Table 3",
    "text": "Table 3\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:\n\n\n\n\n\n\n\n\n\n\nLength of scan suspension\nExcess AAA deaths (excess emergency operations) in Model S1\nLength of time at 7cm threshold\nExcess AAA deaths (excess emergency operations) in Model S3\nDropout rate/ annum\nExcess AAA deaths (excess emergency operations) in Model S2.1\nExcess AAA deaths (excess emergency operations) in Model S2.2\n\n\n\n\n0\n6m\n2 (0)\n6m\n2 (0)\n8%\n46 (24)\n85 (43)\n\n\n1\n12m\n9 (4)\n12m\n10 (4)\n10%\n84 (43)\n153 (79)\n\n\n2\n24m\n43 (23)\n24m\n42 (23)\n12%\n122 (62)\n218 (114)\n\n\n3\n36m\n114 (64)\n36m\n101 (55)\n15%\n176 (91)\n313 (164)\n\n\n4\n48m\n236 (127)\n48m\n179 (98)\nNaN\nNaN\nNaN\n\n\n5\n60m\n409 (223)\n60m\n262 (146)\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "evaluation/reproduction_success.html#supplementary-figure-3",
    "href": "evaluation/reproduction_success.html#supplementary-figure-3",
    "title": "Reproduction success",
    "section": "Supplementary figure 3",
    "text": "Supplementary figure 3\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#supplementary-table-2",
    "href": "evaluation/reproduction_success.html#supplementary-table-2",
    "title": "Reproduction success",
    "section": "Supplementary table 2",
    "text": "Supplementary table 2\nOriginal (Kim et al. (2021)):\n\n\n\n\n\nReproduction:\n\n\n\n\n\n\n\n\n\n\nLength of scan suspension\nScan suspension only (S1)\n+10% dropout/ annum for 1y (S2.1)\n+10% dropout/ annum for 2y (S2.2)\n+7cm threshold for 2y (S3)\n\n\n\n\n0\n6m\n2 (0)\n84 (43)\n153 (79)\n209 (111)\n\n\n1\n12m\n9 (4)\n94 (49)\n164 (85)\n209 (111)\n\n\n2\n24m\n43 (23)\n126 (67)\n209 (112)\n209 (112)\n\n\n3\n36m\n114 (64)\n194 (106)\n274 (147)\n275 (148)\n\n\n4\n48m\n236 (127)\n311 (166)\n385 (207)\n386 (207)\n\n\n5\n60m\n409 (223)\n478 (259)\n547 (297)\n548 (298)"
  },
  {
    "objectID": "evaluation/scope.html",
    "href": "evaluation/scope.html",
    "title": "Scope",
    "section": "",
    "text": "This page outlines the parts of the journal article which we will attempt to reproduce.\nAll images and quotes on this page are sourced from Kim et al. (2021)"
  },
  {
    "objectID": "evaluation/scope.html#within-scope",
    "href": "evaluation/scope.html#within-scope",
    "title": "Scope",
    "section": "Within scope",
    "text": "Within scope\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n‚ÄúFig 1. 65-year-old cohort: Change in key outcomes over varying delay to primary invitation (model I1).‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n\n\n‚ÄúTable 2. Predicted excess AAA deaths and emergency operations in the national invited 65-year-old cohort over 30y period.‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n‚ÄúFig 2. 65-year-old cohort: Change in key outcomes over varying attendance at primary scan (model I2).‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n‚ÄúFig 3. Surveillance cohort: Change in key outcomes over varying suspension of surveillance scans (model S1).‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nIn-text result 1\n\n\n\n\n\nIn bold are the items not captured in Table 3 or Figure 3.\n‚ÄúSuspending ultrasound scans in the surveillance cohort could result in 9 (0.4% increase) additional AAA-related deaths if scans were suspended for one year (Table 3, Fig 3). Of these, 2 (1% increase) are in the sub-group measuring 4.5‚Äì4.9 cm at the start of the pandemic and 7 (8% increase) in the sub-group measuring 5.0‚Äì5.4 cm; &lt;0.1 are in the 3.0‚Äì4.4 cm sub-group. More pronounced effects are evident for suspension for two years and beyond. Suspending surveillance for two years could result in 40 excess AAA-related deaths overall; a 1.9% increase over the lifetime of the surveillance cohort. Of these, 1 is in the 3.0‚Äì4.4 cm sub-group and 17 (7% increase) in the 4.5‚Äì4.9cm sub-group. However, the remaining 22 excess deaths are in the 5.0‚Äì5.4cm range, corresponding to a 24% increase in AAA-related deaths in this sub-group.‚Äù\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n‚ÄúFig 4. Surveillance cohort: Change in key outcomes over varying dropout rates, applied for (i) 1y (model I2.1) and (ii) 2y (model I2.2).‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n‚ÄúFig 5. Surveillance cohort: Change in key outcomes over varying time at increased (7cm) threshold (model I3).‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nTable 3\n\n\n\n\n\n\n\n\n‚ÄúTable 3. Predicted excess AAA deaths and emergency operations in the national surveillance cohort over 30y period.‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nSupplementary figure 3\n\n\n\n\n\n\n\n\n‚ÄúS3 Fig. Cumulative impact of scenarios on surveillance cohort.‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nSupplementary table 2\n\n\n\n\n\n\n\n\n‚ÄúS2 Tab. Predicted excess AAA deaths and emergency operations in the national surveillance cohort over 30y period‚Äù Kim et al. (2021)"
  },
  {
    "objectID": "evaluation/scope.html#outside-scope",
    "href": "evaluation/scope.html#outside-scope",
    "title": "Scope",
    "section": "Outside scope",
    "text": "Outside scope\n\n\n\n\n\n\nTable 1\n\n\n\n\n\nInput parameters and scenarios.\n\n\n\n‚ÄúTable 1. Modelled COVID policy scenarios for AAA services.‚Äù Kim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nSupplementary figure 1\n\n\n\n\n\nPart of methods (rather than a result from the model).\n\n\n\nKim et al. (2021)\n\n\n\n\n\n\n\n\n\n\n\nSupplementary figure 2\n\n\n\n\n\nPart of methods (rather than a result from the model).\n\n\n\nKim et al. (2021)\n\n\n\n\n\n\n\n\n‚ÄúFig 1. 65-year-old cohort: Change in key outcomes over varying delay to primary invitation (model I1).‚Äù Kim et al. (2021)\n‚ÄúTable 2. Predicted excess AAA deaths and emergency operations in the national invited 65-year-old cohort over 30y period.‚Äù Kim et al. (2021)\n‚ÄúFig 2. 65-year-old cohort: Change in key outcomes over varying attendance at primary scan (model I2).‚Äù Kim et al. (2021)\n‚ÄúFig 3. Surveillance cohort: Change in key outcomes over varying suspension of surveillance scans (model S1).‚Äù Kim et al. (2021)\n‚ÄúFig 4. Surveillance cohort: Change in key outcomes over varying dropout rates, applied for (i) 1y (model I2.1) and (ii) 2y (model I2.2).‚Äù Kim et al. (2021)\n‚ÄúFig 5. Surveillance cohort: Change in key outcomes over varying time at increased (7cm) threshold (model I3).‚Äù Kim et al. (2021)\n‚ÄúTable 3. Predicted excess AAA deaths and emergency operations in the national surveillance cohort over 30y period.‚Äù Kim et al. (2021)\n‚ÄúS3 Fig. Cumulative impact of scenarios on surveillance cohort.‚Äù Kim et al. (2021)\n‚ÄúS2 Tab. Predicted excess AAA deaths and emergency operations in the national surveillance cohort over 30y period‚Äù Kim et al. (2021)\n‚ÄúTable 1. Modelled COVID policy scenarios for AAA services.‚Äù Kim et al. (2021)\nKim et al. (2021)\nKim et al. (2021)"
  },
  {
    "objectID": "evaluation/reproduction_report.html",
    "href": "evaluation/reproduction_report.html",
    "title": "Summary report",
    "section": "",
    "text": "Kim LG, Sweeting MJ, Armer M, Jacomelli J, Nasim A, Harrison SC. Modelling the impact of changes to abdominal aortic aneurysm screening and treatment services in England during the COVID-19 pandemic. PLoS ONE 16(6): e0253327 (2021). https://doi.org/10.1371/journal.pone.0253327.\n\nThis study adapts a previously developed DES model for abdominal aortic aneurysm (AAA) screening of men in England. It aims to explore different approaches to resuming screening and surgical repair for AAA, as these survives were paused or substantially reduced during COVID-19 due to concerns about virus transmission."
  },
  {
    "objectID": "evaluation/reproduction_report.html#study",
    "href": "evaluation/reproduction_report.html#study",
    "title": "Summary report",
    "section": "",
    "text": "Kim LG, Sweeting MJ, Armer M, Jacomelli J, Nasim A, Harrison SC. Modelling the impact of changes to abdominal aortic aneurysm screening and treatment services in England during the COVID-19 pandemic. PLoS ONE 16(6): e0253327 (2021). https://doi.org/10.1371/journal.pone.0253327.\n\nThis study adapts a previously developed DES model for abdominal aortic aneurysm (AAA) screening of men in England. It aims to explore different approaches to resuming screening and surgical repair for AAA, as these survives were paused or substantially reduced during COVID-19 due to concerns about virus transmission."
  },
  {
    "objectID": "evaluation/reproduction_report.html#computational-reproducibility",
    "href": "evaluation/reproduction_report.html#computational-reproducibility",
    "title": "Summary report",
    "section": "Computational reproducibility",
    "text": "Computational reproducibility\nSuccessfully reproduced 10 out of 10 (100%) of items from the scope in 13h 59m (35.0%).\nRequired troubleshooting:\n\nReduce run time - reducing population size and setting to parallel to reduce runtime to a feasible level\nFix error in script - fixing a minor error from switching between nested and unnested lists to provide a parameter\nWrite code to implement scenarios and generate outputs - write code to find the aaorta sizes of people with AAA-related deaths, as well as code to produce all of the tables and figures from the model output\n\n\nFigure 1Table 2Figure 2Figure 3In-text result 1Figure 4Figure 5Table 3Supplementary figure 3Supplementary table 2\n\n\n‚ÄúFig 1. 65-year-old cohort: Change in key outcomes over varying delay to primary invitation (model I1).‚Äù Kim et al. (2021)\n \n\n\n‚ÄúTable 2. Predicted excess AAA deaths and emergency operations in the national invited 65-year-old cohort over 30y period.‚Äù Kim et al. (2021)\n\n\n\nOriginal\n\n\n\n\n\n\n\n\n\n\n\n\nLength of delay to invitation\nExcess AAA deaths (excess emergency operations) in Model I1*\nAttendance rate at primary scan\nExcess AAA deaths (excess emergency operations) in Model I2*\n\n\n\n\n0\n6m\n0 (3)\n65%\n61 (32)\n\n\n1\n12m\n0 (0)\n55%\n127 (67)\n\n\n2\n24m\n0 (1)\n45%\n184 (96)\n\n\n3\n36m\n21 (14)\nNaN\nNaN\n\n\n4\n48m\n56 (35)\nNaN\nNaN\n\n\n5\n60m\n108 (56)\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n‚ÄúFig 2. 65-year-old cohort: Change in key outcomes over varying attendance at primary scan (model I2).‚Äù Kim et al. (2021)\n \n\n\n‚ÄúFig 3. Surveillance cohort: Change in key outcomes over varying suspension of surveillance scans (model S1).‚Äù Kim et al. (2021)\n \n\n\nOriginal:\n‚ÄúSuspending ultrasound scans in the surveillance cohort could result in 9 (0.4% increase) additional AAA-related deaths if scans were suspended for one year (Table 3, Fig 3). Of these, 2 (1% increase) are in the sub-group measuring 4.5‚Äì4.9 cm at the start of the pandemic and 7 (8% increase) in the sub-group measuring 5.0‚Äì5.4 cm; &lt;0.1 are in the 3.0‚Äì4.4 cm sub-group. More pronounced effects are evident for suspension for two years and beyond. Suspending surveillance for two years could result in 40 excess AAA-related deaths overall; a 1.9% increase over the lifetime of the surveillance cohort. Of these, 1 is in the 3.0‚Äì4.4 cm sub-group and 17 (7% increase) in the 4.5‚Äì4.9cm sub-group. However, the remaining 22 excess deaths are in the 5.0‚Äì5.4cm range, corresponding to a 24% increase in AAA-related deaths in this sub-group.‚Äù\nReproduction:\n\n\n\n\n\n\n\n\n\n\nyears_of_surveillance_suspension\naorta_size\ndead_scaled\nextra_deaths\npct_change\n\n\n\n\n0\n0\nsmall\n1750\n0\n0.00\n\n\n1\n0\nmed\n225\n0\n0.00\n\n\n2\n0\nlarge\n180\n0\n0.00\n\n\n3\n1\nsmall\n1750\n0\n0.00\n\n\n4\n1\nmed\n227\n2\n0.89\n\n\n5\n1\nlarge\n187\n7\n3.89\n\n\n6\n2\nsmall\n1755\n5\n0.29\n\n\n7\n2\nmed\n235\n10\n4.44\n\n\n8\n2\nlarge\n207\n27\n15.00\n\n\n\n\n\n\n\n\n\n\n‚ÄúFig 4. Surveillance cohort: Change in key outcomes over varying dropout rates, applied for (i) 1y (model I2.1) and (ii) 2y (model I2.2).‚Äù Kim et al. (2021)\n \n\n\n‚ÄúFig 5. Surveillance cohort: Change in key outcomes over varying time at increased (7cm) threshold (model I3).‚Äù Kim et al. (2021)\n \n\n\n‚ÄúTable 3. Predicted excess AAA deaths and emergency operations in the national surveillance cohort over 30y period.‚Äù Kim et al. (2021)\n\n\n\nOriginal\n\n\n\n\n\n\n\n\n\n\n\n\nLength of scan suspension\nExcess AAA deaths (excess emergency operations) in Model S1\nLength of time at 7cm threshold\nExcess AAA deaths (excess emergency operations) in Model S3\nDropout rate/ annum\nExcess AAA deaths (excess emergency operations) in Model S2.1\nExcess AAA deaths (excess emergency operations) in Model S2.2\n\n\n\n\n0\n6m\n2 (0)\n6m\n2 (0)\n8%\n46 (24)\n85 (43)\n\n\n1\n12m\n9 (4)\n12m\n10 (4)\n10%\n84 (43)\n153 (79)\n\n\n2\n24m\n43 (23)\n24m\n42 (23)\n12%\n122 (62)\n218 (114)\n\n\n3\n36m\n114 (64)\n36m\n101 (55)\n15%\n176 (91)\n313 (164)\n\n\n4\n48m\n236 (127)\n48m\n179 (98)\nNaN\nNaN\nNaN\n\n\n5\n60m\n409 (223)\n60m\n262 (146)\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n‚ÄúS3 Fig. Cumulative impact of scenarios on surveillance cohort.‚Äù Kim et al. (2021)\n \n\n\n‚ÄúS2 Tab. Predicted excess AAA deaths and emergency operations in the national surveillance cohort over 30y period‚Äù Kim et al. (2021)\n\n\n\nOriginal\n\n\n\n\n\n\n\n\n\n\n\n\nLength of scan suspension\nScan suspension only (S1)\n+10% dropout/ annum for 1y (S2.1)\n+10% dropout/ annum for 2y (S2.2)\n+7cm threshold for 2y (S3)\n\n\n\n\n0\n6m\n2 (0)\n84 (43)\n153 (79)\n209 (111)\n\n\n1\n12m\n9 (4)\n94 (49)\n164 (85)\n209 (111)\n\n\n2\n24m\n43 (23)\n126 (67)\n209 (112)\n209 (112)\n\n\n3\n36m\n114 (64)\n194 (106)\n274 (147)\n275 (148)\n\n\n4\n48m\n236 (127)\n311 (166)\n385 (207)\n386 (207)\n\n\n5\n60m\n409 (223)\n478 (259)\n547 (297)\n548 (298)"
  },
  {
    "objectID": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "href": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "title": "Summary report",
    "section": "Evaluation against guidelines",
    "text": "Evaluation against guidelines\nNote: this section still needs to be completed\n\n\n\n                                                \n\n\nContext: The original study repository was evaluated against criteria from journal badges relating to how open and reproducible the model is and against guidance for sharing artefacts from the STARS framework. The original study article and supplementary materials (excluding code) were evaluated against reporting guidelines for DES models: STRESS-DES, and guidelines adapted from ISPOR-SDM.\n\n\n\nOriginal\nOriginal\nOriginal"
  },
  {
    "objectID": "logbook/posts/2024_07_26/index.html",
    "href": "logbook/posts/2024_07_26/index.html",
    "title": "Day 1",
    "section": "",
    "text": "Note\n\n\n\nSet-up repository and upload materials. Total time used: 0h 32m (1.3%)"
  },
  {
    "objectID": "logbook/posts/2024_07_26/index.html#set-up-repository",
    "href": "logbook/posts/2024_07_26/index.html#set-up-repository",
    "title": "Day 1",
    "section": "15.58-16.03, 16.21-16.30: Set-up repository",
    "text": "15.58-16.03, 16.21-16.30: Set-up repository\n\nCreate repository from template\nSet up environment\nModified template files:\n\nREADME.md\nquarto site index.qmd\nCITATION.cff\n_quarto.yml\n\nSet up site on GitHub pages (quarto publish gh-pages)"
  },
  {
    "objectID": "logbook/posts/2024_07_26/index.html#upload-code-and-update-license",
    "href": "logbook/posts/2024_07_26/index.html#upload-code-and-update-license",
    "title": "Day 1",
    "section": "16.36-16.41: Upload code and update license",
    "text": "16.36-16.41: Upload code and update license\nModel code is available at: https://github.com/mikesweeting/AAA_DES_model\nIt is available under an LGPL-3.0 license, so I updated this repository to use the same license (GNU LESSER GENERAL PUBLIC LICENSE Version 3). This license requires:\n\nCopy of full license and original copyright notice\nInclude source code when distribute derivative work\nLicense derivative work under same license"
  },
  {
    "objectID": "logbook/posts/2024_07_26/index.html#upload-journal-article",
    "href": "logbook/posts/2024_07_26/index.html#upload-journal-article",
    "title": "Day 1",
    "section": "16.43-16.56: Upload journal article",
    "text": "16.43-16.56: Upload journal article\nThe article is published with PLoS ONE and has a permissive copyright statement:\n\n‚ÄúCopyright: ¬© 2021 Kim et al.¬†This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.‚Äù\n\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253327\nHence, uploaded article, figures, tables and supplementary materials to original_study/. Converted .tif files to .png (e.g.¬†convert sup3.tif sup3.png). Converted .docx to .pdf (libreoffice --headless --convert-to pdf sup1.docx --outdir sup1.pdf).\nAmended study_publication.qmd to display these.\nAdd the study to references.bib."
  },
  {
    "objectID": "logbook/posts/2024_07_26/index.html#timings",
    "href": "logbook/posts/2024_07_26/index.html#timings",
    "title": "Day 1",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('15.58', '16.03'),\n    ('16.21', '16.30'),\n    ('16.36', '16.41'),\n    ('16.43', '16.56')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 32m, or 0h 32m\nTotal used to date: 32m, or 0h 32m\nTime remaining: 2368m, or 39h 28m\nUsed 1.3% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_12/index.html",
    "href": "logbook/posts/2024_08_12/index.html",
    "title": "Day 7",
    "section": "",
    "text": "Note\n\n\n\nWorking on research compendium stage (which inc. troubleshooting some issues with running the model which arose while working on tests)."
  },
  {
    "objectID": "logbook/posts/2024_08_12/index.html#untimed-summary-report",
    "href": "logbook/posts/2024_08_12/index.html#untimed-summary-report",
    "title": "Day 7",
    "section": "Untimed: Summary report",
    "text": "Untimed: Summary report\nAdd required troubleshooting steps to summary report"
  },
  {
    "objectID": "logbook/posts/2024_08_12/index.html#untimed-research-compendium",
    "href": "logbook/posts/2024_08_12/index.html#untimed-research-compendium",
    "title": "Day 7",
    "section": "Untimed: Research compendium",
    "text": "Untimed: Research compendium\n\nInitial steps\nHave seperate folders for data, methods and outputs - have decided there is no need to reorganise - although it is not exactly those three folder names, it does have those different files seperated out\nREADME - filled out the README for the reproduction/ folder (currently partially complete)\nTests - due to the high run time of these scenarios, I created a very minimal test example. This used 65 yo scenario 0 but with 1e3 people (instead of 1e6), and so the test runs in only 5 seconds. This serves as a basic indicator of whether someone‚Äôs model is running and producing results as expected on a new machine, but is in no way comprehensive.\nDockerfile - based on my file from Huang et al.¬†2019. I create and launched a container by running: * sudo docker build --tag kim2021 . -f ./reproduction/docker/Dockerfile * (sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name kim2021_docker kim2021\n\n\nTroubleshooting tests\nI then ran the test (testthat::test_dir(\"tests/testthat\")) in the docker container. However, it returned an error:\nError in `1:v0$numberOfPersons`: argument of length 0\nBacktrace:\n    ‚ñÜ\n 1. ‚îî‚îÄglobal Eventsandcosts(scen0.invite) at test-model.R:50:3\n 2.   ‚îú‚îÄbase::unlist(...) at functions/DES_Model.R:1684:5\n 3.   ‚îî‚îÄbase::sapply(...) at functions/DES_Model.R:1684:5\n 4.     ‚îî‚îÄbase::lapply(X = X, FUN = FUN, ...)\nI tried changing the number of people (1e4), and then the seed (3210 to 999), but the issue persisted within the docker container.\nLooking at the variable v0, on the container it becomes:\ngenerateCensoringTime = \nfunction() { 30.000001 }\n&lt;bytecode: 0x59655b5310c8&gt;\nHowever, when running tests on my machine, its value is:\ngenerateCensoringTime = \nfunction() { 30.000001 }\n&lt;bytecode: 0x652354295330&gt;\ntreatmentGroups = screening \nreturnEventHistories = TRUE \nreturnAllPersonsQuantities = FALSE \nmethod = serial \nnumberOfPersons = 1000 \nrandomSeed = 3210 \nI tried clearing the environment on my machine and re-running the test, and it returned the same issue - and so it seems the issue is unrelated to docker, but instead was unnoticed prior as I hadn‚Äôt cleared my environment.\nPrinting v0 during the test, I can see it is set up correctly. I add the original print statements from the scenario script and then found I got a different error:\n&lt;subscriptOutOfBoundsError/error/condition&gt;\nError in `result$eventHistories[[i]]`: subscript out of bounds\nBacktrace:\n    ‚ñÜ\n 1. ‚îî‚îÄglobal Eventsandcosts(scen0.invite) at test-model.R:58:3\n 2.   ‚îú‚îÄbase::unlist(...) at functions/DES_Model.R:1684:5\n 3.   ‚îî‚îÄbase::sapply(...) at functions/DES_Model.R:1684:5\n 4.     ‚îî‚îÄbase::lapply(X = X, FUN = FUN, ...)\n 5.       ‚îî‚îÄFUN(X[[i]], ...)\nI checked the code matched the original, and tried tweaking little bits and bobs, but found the error persisted.\nI tried adding a print statement for components going into the formula, where the error is occuring, in DES_Model.R -\nprint(v0$numberOfPersons)\nnoScreening.events&lt;-\n  unlist(sapply(1:v0$numberOfPersons,function(i){result$eventHistories[[i]]$noScreening$events}))\n\nand I discovered that its value was set to 1e6 (when it should now be 1e3). I struggled to identify why this had occurred, but tried changing 1:v0$numberOfPersons to 1:length(result$eventHistories) (as these should just be the same thing). This resolved the issue!\n\nI rebuilt the docker image and launched a container and ran the test, but it failed again, this time with the error -\nError in `x[[jj]][iseq] &lt;- vjj`: replacement has length zero\nBacktrace:\n    ‚ñÜ\n 1. ‚îî‚îÄglobal Eventsandcosts(scen0.invite) at test-model.R:57:3\n 2.   ‚îú‚îÄbase::`[&lt;-`(`*tmp*`, i, \"screening.prev\", value = `&lt;dbl&gt;`) at functions/DES_Model.R:1695:9\n 3.   ‚îî‚îÄbase::`[&lt;-.data.frame`(`*tmp*`, i, \"screening.prev\", value = `&lt;dbl&gt;`) at functions/DES_Model.R:1695:9\nGoing to that line in DES_Model.R, I can see that the error is again related to v0$numberOfPersons:\nEventsandcosts.df[i,\"screening.prev\"]&lt;-\n          Eventsandcosts.df[i,\"screening.n\"]/v0$numberOfPersons\nThe value for v0$numberOfPersons is correct when printed from the test, but incorrect when running through the Eventsandcosts() function. The issue appears to be that Eventsandcosts() doesn‚Äôt accept v0 as an input parameter, and instead uses it like a global parameter - but presumably, within a test, this is leading to issues.\nI attempted a wrap around solution, which was to calculate the number of persons from length(result$eventHistories) and use that in place of all v0$numberOfPersons. I cleared my environment and re-ran the test: it appeared to work fine.\nI rebuilt docker image and ran test in container - which passed!\nTo make sure this hadn‚Äôt inadvertently affected my normal scenario scripts in anyway, I re-ran surveillance scenario 0, confirming the output .csv had not changed. Of note, the run time on that on my machine (rather than the high spec remote machine) was 21 minutes 59 seconds (whilst on remote it was 4 minutes 28 seconds). The main model output was the same, but the AAA-related death aaorta sizes did change. I‚Äôm not sure why one remained the same and one changed. I ran this again on the remote machine this time (Rscript -e \"source('models/NAAASP_COVID_modelling/run_aaamodel_surv_scen0.R')\"). And there was no change in the file! Given the motivations of this reproduction and stage we are at, I will not troubleshoot further on this issue.\n\n\nFinal steps\nGitHub Container Registry (GHCR): Activated GitHub action to push Docker image to GitHub container registry. Pulled this, open RStudio, and ran test which passed.\nRun times: This had largely been completed during reproduction, but I add times for the test, and the time from running surv scen 0 on a different machine (as above).\nQuarto site: Finally, I modified the GitHub action. So far, it had set to not build .Rmd files (to avoid errors encountered due to R environment). However, as I would like to include the notebook in the final Quarto site, I then modified the action as I did in Huang et al.¬†to not run with every push, and instead manually updated the site from my local macine using quarto publish. This is a temporary fix, as I have not been able to get a GitHub action working to render this on pushes automatically for me, for R. When doing so, have to remember to delete _freeze/, _site/ and .quarto before running, to ensure it runs everything properly."
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html",
    "href": "logbook/posts/2024_08_01/index.html",
    "title": "Day 5",
    "section": "",
    "text": "Note\n\n\n\nFinished runnning scenarios, and reproduced in-text result 1, figure 5 and table 3. Total time used: 13h 25m (33.5%)."
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#get-aorta-sizes-from-aaa-deaths-in-surv-scenario-0",
    "href": "logbook/posts/2024_08_01/index.html#get-aorta-sizes-from-aaa-deaths-in-surv-scenario-0",
    "title": "Day 5",
    "section": "09.12-09.19, 09.22-09.24: Get aorta sizes from AAA deaths in surv scenario 0",
    "text": "09.12-09.19, 09.22-09.24: Get aorta sizes from AAA deaths in surv scenario 0\nCreate a script functions/extra_functions.R and moved the function I created yesterday - get_aaa_death_aorta_sizes() - from run_aaamodel_surv_scen1.R to extra_functions.R. This is so that:\n\nIt sits alongside other model functions\nIt can be used in scenario 0 and scenario 1 without duplication\n\nI test ran it on a small sample before setting it to run on a remote machine.\nTimings:\n\nFull run (one scenario): 4.633 minutes = 278 seconds = 4 minutes 38 seconds"
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#set-surv-scenario-4c-e-to-run",
    "href": "logbook/posts/2024_08_01/index.html#set-surv-scenario-4c-e-to-run",
    "title": "Day 5",
    "section": "09.34-09.35, 10.28-10.29, 11.05-11.07: Set surv scenario 4C-E to run",
    "text": "09.34-09.35, 10.28-10.29, 11.05-11.07: Set surv scenario 4C-E to run\nRunning on the remote machine‚Ä¶\nTimings for 4C:\n\nFirst run: 5.452 minutes = 327 seconds = 5 minutes 27 seconds\nAll runs: 35.234 minutes = 2114 seconds = 35 minutes 14 seconds\n\nTimings for 4D:\n\nFirst run: 5.429 minutes = 326 seconds = 5 minutes 26 seconds\nAll runs: 35.083 minutes= 2105 seconds = 35 minutes 5 seconds\n\nTimings for 4E:\n\nFirst run: 5.465 minutes = 328 seconds = 5 minutes 28 seconds\nAll runs: 34.994 minutes = 2096 seconds = 34 minutes 56 seconds\n\nAlso ran some others again on the remote machine that were missing run times:\n\n65yo scen 0: 3.528 min = 212 seconds = 3 minutes 32 seconds\n65yo scen 1: 29.230 min = 1754 seconds = 29 minutes 14 seconds\nSurv scen 4b: 34.955 min = 2097 seconds = 34 minutes 57 seconds\n\nReassuringly, for those re-run, no change in results observed (so seed control all working as anticipated)."
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#in-text-result-1",
    "href": "logbook/posts/2024_08_01/index.html#in-text-result-1",
    "title": "Day 5",
    "section": "15.42-16.02: In-text result 1",
    "text": "15.42-16.02: In-text result 1\nCombined results from my function added to surv scenario 0 and 1, to view deaths by aorta size. The numbers are pretty similar to the original - exacty the same for one year (0, 2 and 7 deaths in the small, medium and large groups repsectively). They differed a little more for the suspension over 2 years:\n\nSmall: Original 1, Mine 5\nMedium: Original 17, Mine 10\nLarge: Original 22, Mine 27\n\nHowever, given the reduced sample size we run, I would consider this reasonably reproduced (with a consistent pattern, and relatively similar numbers), although will see what others say when send for a consensus opinion.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 741\n\n# Times from today\ntimes = [\n    ('09.12', '09.19'),\n    ('09.22', '09.24'),\n    ('09.34', '09.35'),\n    ('10.28', '10.29'),\n    ('11.05', '11.07'),\n    ('15.42', '16.02')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 33m, or 0h 33m\nTotal used to date: 774m, or 12h 54m\nTime remaining: 1626m, or 27h 6m\nUsed 32.2% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#figure-5",
    "href": "logbook/posts/2024_08_01/index.html#figure-5",
    "title": "Day 5",
    "section": "16.06-16.17: Figure 5",
    "text": "16.06-16.17: Figure 5\nThe model output from surv scen 3 includes thres 5.5 period 0, and then thresh 7 for periods 0.5 +. This aligns with the description in Table 1 in the paper. Hence, I‚Äôm assuming that calculations are in comparison against 5.5 period 0 (and that there is not a thresh 7 period 0 anywhere, since thresh 5.5 is otherwise the default).\nFeel this is reproduced at 16.17.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 741\n\n# Times from today\ntimes = [\n    ('09.12', '09.19'),\n    ('09.22', '09.24'),\n    ('09.34', '09.35'),\n    ('10.28', '10.29'),\n    ('11.05', '11.07'),\n    ('15.42', '16.02'),\n    ('16.06', '16.17')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 44m, or 0h 44m\nTotal used to date: 785m, or 13h 5m\nTime remaining: 1615m, or 26h 55m\nUsed 32.7% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#finishing-up-table-3",
    "href": "logbook/posts/2024_08_01/index.html#finishing-up-table-3",
    "title": "Day 5",
    "section": "16.19-16.26: Finishing up Table 3",
    "text": "16.19-16.26: Finishing up Table 3\nCreated the final section of table 3 from scenario 3, and then combined with prior sections to produced Table 3.\nFeel this is reproduced at 16.26.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 741\n\n# Times from today\ntimes = [\n    ('09.12', '09.19'),\n    ('09.22', '09.24'),\n    ('09.34', '09.35'),\n    ('10.28', '10.29'),\n    ('11.05', '11.07'),\n    ('15.42', '16.02'),\n    ('16.06', '16.17'),\n    ('16.19', '16.27')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 52m, or 0h 52m\nTotal used to date: 793m, or 13h 13m\nTime remaining: 1607m, or 26h 47m\nUsed 33.0% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#supplementary-table-2",
    "href": "logbook/posts/2024_08_01/index.html#supplementary-table-2",
    "title": "Day 5",
    "section": "16.40-16.53: Supplementary table 2",
    "text": "16.40-16.53: Supplementary table 2\nLooking through the code and results:\n\nScenario 4A has scan suspension + 10% dropout over 1 year\nScenario 4B has scan suspension + 10% dropout over 2 years\nScenario 4C has scan suspension + 10% dropout over 2 years + 7cm threshold for 2 years\nScenario 4D has scan suspension + 10% dropout over 2 years + 7cm threshold for 2 years + 2 lockdown (op suspension) periods\nScenario 4E has scan suspension + 10% dropout over 2 years + 7cm threshold for 3 months and 5.5cm threshold for 3 months\n\nSupplementary table 1 and supplementary figure 3 appears to include just 4A to 4C, plus a prior scenario that just had scan suspension. Hence, I removed their run times from the README.\nScan suspension is presented in Figure 3, which uses scenarios 0 and 1."
  },
  {
    "objectID": "logbook/posts/2024_08_01/index.html#timings",
    "href": "logbook/posts/2024_08_01/index.html#timings",
    "title": "Day 5",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 741\n\n# Times from today\ntimes = [\n    ('09.12', '09.19'),\n    ('09.22', '09.24'),\n    ('09.34', '09.35'),\n    ('10.28', '10.29'),\n    ('11.05', '11.07'),\n    ('15.42', '16.02'),\n    ('16.06', '16.17'),\n    ('16.19', '16.26'),\n    ('16.40', '16.53')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 64m, or 1h 4m\nTotal used to date: 805m, or 13h 25m\nTime remaining: 1595m, or 26h 35m\nUsed 33.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html",
    "href": "logbook/posts/2024_07_30/index.html",
    "title": "Day 3",
    "section": "",
    "text": "Note\n\n\n\nReproduced Figure 1 and Table 2. Total time used: 8h 59m (22.5%)."
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#timings-below-working-on-table-2-and-figure-1",
    "href": "logbook/posts/2024_07_30/index.html#timings-below-working-on-table-2-and-figure-1",
    "title": "Day 3",
    "section": "Timings below: Working on Table 2 and Figure 1",
    "text": "Timings below: Working on Table 2 and Figure 1\nTimings:\n\n09.10-09.11\n09.20-09.22\n09.30-09.40\n09.52-10.44\n10.51-10.56\n11.02-11.56\n12.59-13.27\n\n\nGitHub repository dates\nI realised I had forgotten to check the GitHub repository dates v.s. paper dates. The article was published in June 2021. There are three commits after this point:\n\n19 July 2021 - ‚ÄúAdding SWAN models to repository‚Äù - relevant changes:\n\ndebug=F and some error handling in processPersons() from DES_Model.R\nChanging from v1other$aortaDiameterThresholds &lt;- c(3.0, 4.5, 5.5) to v1other$aortaDiameterThresholds &lt;- list(c(3.0, 4.5, 5.5)) in input/NAAASP_Men_2020-05-11/DES_Data_Input_NAAASP_Men_30years_time_horizon_2020-05-11.R\nRemoving an associated .xlsx file for 202-05-11\n\n19 July 2021 - ‚ÄúUpdating README.md‚Äù\n\nMore detail in README (see below)\n\n12 February 2024 - ‚ÄúFixed checks with vectorised elements, which was causing errors in R v4.3‚Äù\n\nAdd .gitignore\nModification in Auxillary_Functions.R\nFormatting in DES_Model.R\n\n\nOriginal:\n\nThe following directories are included:\n/models ‚Äì R scripts that run the DES models for AAA screening\n/models/Example ‚Äì An example script to run the DES is contained here\n/functions ‚Äì Contains the DES model code\n/input ‚Äì Contains the input parameters and .csv files needed to run the DES models\n/input/NAAASP_Men_2020-05-11 ‚Äì Updated parameters for AAA screening in men, updated as of 11/05/2020\n/output ‚Äì Directory where Rdata output files are saved\n\nNew:\n\nThe following directories are included:\n\n/models ‚Äì R scripts that run the DES models for AAA screening\n/models/Example ‚Äì An example script to run the DES is contained here\n/models/SWAN ‚Äì Screening Women for Abdominal Aortic Aneurysm (SWAN) model scripts. See our Lancet publication and HTA report for further details\n/models/NAAASP_COVID_modelling ‚Äì Scripts for modelling the impact of changes to Abdominal Aortic Aneurysm screening and treatment services in England during the COVID-19 pandemic. See our PLOS ONE publication for further details\n/functions ‚Äì Contains the DES model code\n/input ‚Äì Contains the input parameters and .csv files needed to run the DES models\n/input/SWAN ‚Äì Screening Women for Abdominal Aortic Aneurysm (SWAN) input parameters. See our Lancet publication and HTA report for further details\n/input/NAAASP_COVID_modelling ‚Äì Input parameters for modelling the impact of changes to Abdominal Aortic Aneurysm screening and treatment services in England during the COVID-19 pandemic. See our PLOS ONE publication for further details\n/input/NAAASP_Men_2020-05-11 ‚Äì Updated parameters for AAA screening in men, updated as of 11/05/2020\n/output ‚Äì Directory where Rdata output files are saved\n\n\n\n\n\n\n\n\nReflection\n\n\n\nGiven how far I have progressed with this, I won‚Äôt go back to using the version as of publication, but we should bare this in mind later.\n\n\n\n\nIncrementing number of people for 65yo scen1\nRan full script of run_aaamodel-65yo_scen1.R with 100,000 people and parallel. However, this evidently is still too few people, looking at the results.\n\nimport pandas as pd\n\npd.read_csv('tab2_100k.csv')\n\n\n\n\n\n\n\n\n\ndelayscr\nexcess_death\nexcess_op\n\n\n\n\n0\n0.00\n0\n0\n\n\n1\n0.25\n-6\n-3\n\n\n2\n0.50\n-5\n3\n\n\n3\n1.00\n-6\n-1\n\n\n4\n2.00\n-1\n-3\n\n\n5\n3.00\n2\n3\n\n\n6\n4.00\n25\n7\n\n\n7\n5.00\n41\n7\n\n\n\n\n\n\n\n\nI pulled changes to files and renv onto remote computer, and then ran that with 1,000,000 people. To open and change file:\nnano 'models/NAAASP_COVID_modelling/run_aaamodel_65yo_scen1.R'\nTo run:\nRscript -e \"source('models/NAAASP_COVID_modelling/run_aaamodel_65yo_scen1.R')\"\nPer run this took:\n\n3 minutes 46 seconds on remote computer - so estimated 30 minutes in total.\n13 minutes 44 seconds on local computer - so would be an estimated 1 hour 50 minutes in total\n\nLooking at the convergence plots (supplementary figure 1 and 2), 1 million people is nearing closer to convergence - but 2 million even more-so.\n\nimport pandas as pd\n\npd.read_csv('tab2_1m.csv')\n\n\n\n\n\n\n\n\n\ndelayscr\naaadead\nemeropen\nexcess_death\nexcess_op\n\n\n\n\n0\n0.00\n9226\n2830\n0\n0\n\n\n1\n0.25\n9200\n2824\n-26\n-6\n\n\n2\n0.50\n9194\n2843\n-32\n13\n\n\n3\n1.00\n9199\n2830\n-27\n0\n\n\n4\n2.00\n9210\n2836\n-16\n6\n\n\n5\n3.00\n9298\n2878\n72\n48\n\n\n6\n4.00\n9423\n2933\n197\n103\n\n\n7\n5.00\n9612\n2993\n386\n163\n\n\n\n\n\n\n\n\n\n\nScaling the numbers\nLooking at the result, I‚Äôm a little uncertain over the numbers we are getting. Looking at Table 2‚Äôs caption, I‚Äôm thinking that perhaps I may need to scale these (as using the raw numbers from the simulation, we‚Äôd expect these to be less than the table). The caption mentions that the:\n\nNational male 65 year old cohort for England: n = 279,798\nExpected AAA deaths over 30y in status quo = 2564\nExpected emergency operations over 30y in status quo = 1041\n\nI tried scaling the results so that it reflects deaths expected if population were 279,798 (rather than 1 million). I scaled the results from aaadead and emerevar (e.g.¬†round(279798*(aaadead/1000000))). The number of deaths looks similar to the expected from Table 2 (2564), but the number of emergency operations is very different.\n\nimport pandas as pd\n\npd.read_csv('scaled_1m.csv')\n\n\n\n\n\n\n\n\n\ndelayscr\naaadead\nscaled_dead\nscaled_emer\n\n\n\n\n0\n0.00\n9226\n2581\n261\n\n\n1\n0.25\n9200\n2574\n261\n\n\n2\n0.50\n9194\n2572\n261\n\n\n3\n1.00\n9199\n2574\n261\n\n\n4\n2.00\n9210\n2577\n261\n\n\n5\n3.00\n9298\n2602\n261\n\n\n6\n4.00\n9423\n2637\n261\n\n\n7\n5.00\n9612\n2689\n261\n\n\n\n\n\n\n\n\n\n\nScenario 0\nI was wondering whether this should be compared against the result from the status quo script rather than from when delay is 0 (or whether that should just be the same thing anyway!).\nI repeated this for the results from scenario 0 (status quo):\nscale_dead_s0 &lt;- y65_s0 %&gt;%\n  select(aaadead) %&gt;%\n  mutate(scaled_dead = round(279798*(aaadead/1000000)))\nwrite.csv(scale_dead_s0, \"../../logbook/posts/2024_07_30/dead_s0.csv\", row.names=FALSE)\nIt came out exactly the same, so seems, no issue with using the 0 result from the run of scen1.\n\nimport pandas as pd\n\npd.read_csv('dead_s0.csv')\n\n\n\n\n\n\n\n\n\naaadead\nscaled_dead\n\n\n\n\n0\n9226\n2581\n\n\n\n\n\n\n\n\n\n\nConfirming which column gives number of operations\nGiven that number of emergency operations looked very different, I was suspicious if I was using the correct column for this. It seems clear that deaths and ruptures are aaadead and rupt respectively, but I was less certain for surgeries (elecopen vs elecevar) (emeropen vs emerevar).\nI tried applying scaling to every column from row 1, to see which came out most similar.\nas.data.frame(t(head(y65_s1, 1))) %&gt;%\n  rename(result = 1) %&gt;%\n  mutate(scaled = round(279798*(result/n_person)))\nBased on the names, there are a few candidates for emergency operations: emerevar, emeropen, reintemerevar and reintemeropen. I‚Äôm not certain what the difference is between these, but starting with their scaled values:\n\nemerevar - 242\nemeropen - 790\nreintemerevar - 935\nreintemeropen - 183\n\nThe expected number is 1041, so reintemerevar does appear closest.\n\nimport pandas as pd\n\npd.read_csv('scale_all_first_row.csv', index_col=0)\n\n\n\n\n\n\n\n\n\nresult\nscaled\n\n\n\n\nn\n1000000.00\n279798\n\n\ndelayscr\n0.25\n0\n\n\ninv\n997073.00\n278979\n\n\nscr\n748352.00\n209387\n\n\nreinv\n135543.00\n37925\n\n\nnonatt\n248721.00\n69592\n\n\nmonitor\n265786.00\n74366\n\n\ndropout\n9445.00\n2643\n\n\noppdet\n28438.00\n7957\n\n\nconsult\n25555.00\n7150\n\n\nelecevar\n10721.00\n3000\n\n\nelecopen\n4739.00\n1326\n\n\nrupt\n10093.00\n2824\n\n\nemerevar\n865.00\n242\n\n\nemeropen\n2824.00\n790\n\n\nreintelecevar\n3340.00\n935\n\n\nreintemerevar\n3340.00\n935\n\n\nreintemeropen\n655.00\n183\n\n\naaadead\n9200.00\n2574\n\n\nnonaaadead\n909861.00\n254577\n\n\n\n\n\n\n\n\nI also then looked into the repository, for any written explanation for each variable, or the code behind it, to help confirm which is appropriate. Looking across the repository, these columns are created within each of the scenario scripts. From run_aaamodel_65yo_scen1.R:\nelecevar&lt;-Eventsandcosts(scen1.invite)[14,2]\nelecopen&lt;-Eventsandcosts(scen1.invite)[15,2]\n\nemerevar&lt;-Eventsandcosts(scen1.invite)[17,2]\nemeropen&lt;-Eventsandcosts(scen1.invite)[18,2]\n\nreintelecevar&lt;-Eventsandcosts(scen1.invite)[21,2]\nreintemerevar&lt;-Eventsandcosts(scen1.invite)[21,2]\nLooking directly at the output of Eventsandcosts(scen1.invite) I can see that these match up with:\n\n14 - electiveSurgeryEvar\n15 - electiveSurgeryOpen\n17 - emergencySurgeryEvar\n18 - emergencySurgeryOpen\n21- reinterventionAfterElectiveEvar\n\nI‚Äôm not certain why reintemerevar uses 21 and not 22, as 22 is reinterventionAfterEmergencyEvar, but this appears to be the same in each of the scenario scripts.\nThese are assigned from costs, and v2$costs is in the data dictionary, DES_Input_Definitions.xlsx:\n\nelectiveSurgeryEvar: Elective EVAR repair\nelectiveSurgeryOpen: Elective Open repair\nemergencySurgeryEvar: Emergency EVAR repair\nemergencySurgeryOpen: Emergency Open repair\nreinterventionAfterElectiveEvar: Re-intervention after elective EVAR\n\nHaving looked at this, I am presuming that the number of emergency operations would be given by combining emergencySurgeryEvar (17) (emerevar) and emergencySurgeryOpen (18) (emeropen).\nOnce I did that, the numbers looked much closer to the expected 1041 from the paper, with 1034 from 1 million people, and 1028 from 2 million people.\n\n\nExcess deaths\nI‚Äôm seeing some negatives (but the paper does not, and has several 0). I‚Äôm wondering if, as the result is ‚Äúexcess‚Äù deaths, perhaps I should only be counting when it was over 0 (and setting negative numbers to 0), so I altered it accordingly.\nI also trimmed down the rows displayed for Table 2, to make the article.\n\n\nTwo million people\nI ran run_aaamodel_65yo_scen1.R again but with 2 million people, as that looks like a more appropriate figure from the convergence plots, but I wasn‚Äôt certain if:\n\n\nthe remote computer would manage it (process killed when ran with 10 million)\n\n\nhow long it would take\n\n\nhow much of a difference it would make\n\n\nRE: (a) - it did manage it. While running, I did note we are now getting very large objects from each run (e.g.¬†processPersons is about to return an object of size 4.5 Gb)\nRE: (b) - the time for one run with 2 million people was 11 minutes 20 seconds. For all eight runs, it took a total of ‚Äú1.01512100968096‚Äù (which is weird - normally it gives me a result in minutes). If we assume this is hours, it‚Äôs just over an hour. My estimate based on the time for one run would be 1 hour 30 minutes. Have changed the timing statement to use difftime instead with units specified to avoid this issue in future.\nRE: (c) - the results are pretty similar. I renamed the output files to we had results with 1 million and 2 million, then applied same function to create table 2, and they were pretty similar.\n\nimport pandas as pd\n\ndisplay(pd.read_csv('tab2_correct_1m.csv'))\ndisplay(pd.read_csv('tab2_correct_2m.csv'))\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead\nexcess_emer\n\n\n\n\n0\n6m\n0\n3\n\n\n1\n12m\n0\n0\n\n\n2\n24m\n0\n1\n\n\n3\n36m\n21\n14\n\n\n4\n48m\n56\n35\n\n\n5\n60m\n108\n56\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead\nexcess_emer\n\n\n\n\n0\n6m\n0\n0\n\n\n1\n12m\n0\n0\n\n\n2\n24m\n0\n0\n\n\n3\n36m\n20\n11\n\n\n4\n48m\n50\n31\n\n\n5\n60m\n105\n60\n\n\n\n\n\n\n\n\nGiven their similarity, I will continue with running the simulation with 1 million people for the time being, since running with 2 million was much longer and very computationally expensive."
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#running-65-year-old-scenario-2",
    "href": "logbook/posts/2024_07_30/index.html#running-65-year-old-scenario-2",
    "title": "Day 3",
    "section": "13.29-13.39: Running 65 year old scenario 2",
    "text": "13.29-13.39: Running 65 year old scenario 2\nScenario 2 has a delay to invitation of 6 months, and attendance of 45 to 75%. This is required for Table 2, which also displays results from attendance of 65%, 55% and 45% (compared against 75%).\nI set this up to run on the remote computer, with the changes as before, of:\n\nUsing parallel\n1 million\nSaving the results to csv\nCommenting out list(v1other$aortaDiameterThresholds)\nTimings for one run and all runs\n\nTimings:\n\nOne run: 3.952 minutes = 237 seconds = 3 minutes 57 seconds\nFull run: 19.123 minutes = 1147 seconds = 19 minutes 7 seconds"
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#running-surv-scenario-1",
    "href": "logbook/posts/2024_07_30/index.html#running-surv-scenario-1",
    "title": "Day 3",
    "section": "14.00-14.06: Running surv scenario 1",
    "text": "14.00-14.06: Running surv scenario 1\nMidway through creating figure 1, I set remote computer to run another scenario.\nAmended script as did for 65yo scen2, with an additional fix too - correct path to model script to DES_Model.R - and then ran on remote machine.\nTimings:\n\nOne run: 13.368 minutes = 802 seconds = 13 minutes 22 seconds\nFull run: 70.183 minutes = 4211 seconds = 1 hour 10 minutes 11 seconds\n\nThis runtime reaffirmed for me the use of 1,000,000 people (rather than 2,000,000)."
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#creating-figure-1",
    "href": "logbook/posts/2024_07_30/index.html#creating-figure-1",
    "title": "Day 3",
    "section": "13.40-13.44, 13.51-13.59, 14.07-14.29: Creating Figure 1",
    "text": "13.40-13.44, 13.51-13.59, 14.07-14.29: Creating Figure 1\nWhilst that ran - and then, setting others to run during this too (hence time jumps) - I set about creating Figure 1 from the results of scenario 1 with 1 million people.\nAdd ggplot2 (to make plots) and tidyr (to melt dataframe) to renv.\nWrote code to create figure, and was satisified this was reproduced at 14.29.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 243\n\n# Times from today\ntimes = [\n    ('09.10', '09.11'),\n    ('09.20', '09.22'),\n    ('09.30', '09.40'),\n    ('09.52', '10.44'),\n    ('10.51', '10.56'),\n    ('11.02', '11.56'),\n    ('12.59', '13.27'),\n    ('13.29', '13.39'),\n    ('13.40', '13.44'),\n    ('13.51', '13.59'),\n    ('14.00', '14.06'),\n    ('14.07', '14.29')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 202m, or 3h 22m\nTotal used to date: 445m, or 7h 25m\nTime remaining: 1955m, or 32h 35m\nUsed 18.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#checking-scenario-1-simulation-table-2-results",
    "href": "logbook/posts/2024_07_30/index.html#checking-scenario-1-simulation-table-2-results",
    "title": "Day 3",
    "section": "14.42-14.49: Checking scenario 1 simulation table 2 results",
    "text": "14.42-14.49: Checking scenario 1 simulation table 2 results\nNow that we have the function to process scenario 1 correctly, I went back to re-run the results from fewer numbers of people, just to reaffirm whether the chosen number is appropriate (or whether we could use fewer).\nI downloaded the results from my GitHub commit history, renaming the files to indicating the number of people.\nWith each of the variants I‚Äôd tried, it is apparent that none below 1 million were appropriate. It is possible that there could have been an appropriate inbetween value that I hadn‚Äôt tried (e.g.¬†500,000), but for now, I will stick with 1 million.\n\nimport pandas as pd\n\ndisplay(pd.read_csv('65y_s1_tab2_1k.csv'))\ndisplay(pd.read_csv('65y_s1_tab2_10k.csv'))\ndisplay(pd.read_csv('65y_s1_tab2_100k.csv'))\ndisplay(pd.read_csv('65y_s1_tab2_1m.csv'))\ndisplay(pd.read_csv('65y_s1_tab2_2m.csv'))\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead_emer\n\n\n\n\n0\n6m\n0 (NA)\n\n\n1\n12m\n0 (NA)\n\n\n2\n24m\n0 (NA)\n\n\n3\n36m\n0 (NA)\n\n\n4\n48m\n279 (NA)\n\n\n5\n60m\n279 (NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead_emer\n\n\n\n\n0\n6m\n56 (0)\n\n\n1\n12m\n0 (0)\n\n\n2\n24m\n56 (0)\n\n\n3\n36m\n56 (0)\n\n\n4\n48m\n140 (0)\n\n\n5\n60m\n196 (0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead_emer\n\n\n\n\n0\n6m\n0 (6)\n\n\n1\n12m\n0 (0)\n\n\n2\n24m\n0 (0)\n\n\n3\n36m\n5 (6)\n\n\n4\n48m\n70 (25)\n\n\n5\n60m\n114 (25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead_emer\n\n\n\n\n0\n6m\n0 (3)\n\n\n1\n12m\n0 (0)\n\n\n2\n24m\n0 (1)\n\n\n3\n36m\n21 (14)\n\n\n4\n48m\n56 (35)\n\n\n5\n60m\n108 (56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonths\nexcess_dead_emer\n\n\n\n\n0\n6m\n0 (0)\n\n\n1\n12m\n0 (0)\n\n\n2\n24m\n0 (0)\n\n\n3\n36m\n20 (11)\n\n\n4\n48m\n50 (31)\n\n\n5\n60m\n105 (60)"
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#table-2",
    "href": "logbook/posts/2024_07_30/index.html#table-2",
    "title": "Day 3",
    "section": "15.00-15.59: Table 2",
    "text": "15.00-15.59: Table 2\nModified function so it could be used to process both scenarios, then combined into a single table.\nI was satisfied this was reproduced (within expected variation from running with less people) at 15.59.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 243\n\n# Times from today\ntimes = [\n    ('09.10', '09.11'),\n    ('09.20', '09.22'),\n    ('09.30', '09.40'),\n    ('09.52', '10.44'),\n    ('10.51', '10.56'),\n    ('11.02', '11.56'),\n    ('12.59', '13.27'),\n    ('13.29', '13.39'),\n    ('13.40', '13.44'),\n    ('13.51', '13.59'),\n    ('14.00', '14.06'),\n    ('14.07', '14.29'),\n    ('14.42', '14.49'),\n    ('15.00', '15.59')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 268m, or 4h 28m\nTotal used to date: 511m, or 8h 31m\nTime remaining: 1889m, or 31h 29m\nUsed 21.3% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#fix-later-scenarios",
    "href": "logbook/posts/2024_07_30/index.html#fix-later-scenarios",
    "title": "Day 3",
    "section": "16.12-16.27: Fix later scenarios",
    "text": "16.12-16.27: Fix later scenarios\nFixed surv scenarios 2 to 4e (although not yet running as potentially insufficient time to complete before end of work day). As above, fixes include:\n\nUsing parallel\n1 million\nSaving the results to csv\nCommenting out list(v1other$aortaDiameterThresholds)\nTimings for one run and all runs\nCorrecting filename when source DES_Model.R\n\n\n\n\n\n\n\nReflection\n\n\n\nI haven‚Äôt modified the provided structure of seperate scripts, but this structure does make it challenging when you want to change a parameter across the runs, as you have to carefully change each of the scripts. It would be easier if they were controlled by a single set of shared parameters."
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#start-on-figure-2",
    "href": "logbook/posts/2024_07_30/index.html#start-on-figure-2",
    "title": "Day 3",
    "section": "16.42-16.55: Start on Figure 2",
    "text": "16.42-16.55: Start on Figure 2\nCreate function based on Figure 1, so can reuse same processing code to generate dataframe for the figure."
  },
  {
    "objectID": "logbook/posts/2024_07_30/index.html#timings",
    "href": "logbook/posts/2024_07_30/index.html#timings",
    "title": "Day 3",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 243\n\n# Times from today\ntimes = [\n    ('09.10', '09.11'),\n    ('09.20', '09.22'),\n    ('09.30', '09.40'),\n    ('09.52', '10.44'),\n    ('10.51', '10.56'),\n    ('11.02', '11.56'),\n    ('12.59', '13.27'),\n    ('13.29', '13.39'),\n    ('13.40', '13.44'),\n    ('13.51', '13.59'),\n    ('14.00', '14.06'),\n    ('14.07', '14.29'),\n    ('14.42', '14.49'),\n    ('15.00', '15.59'),\n    ('16.12', '16.27'),\n    ('16.42', '16.55')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 296m, or 4h 56m\nTotal used to date: 539m, or 8h 59m\nTime remaining: 1861m, or 31h 1m\nUsed 22.5% of 40 hours max"
  }
]